{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# utility\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from data_cleaning import clean_raw_data, create_dataset, get_all_results\n",
    "# parallel\n",
    "import ray\n",
    "try:\n",
    "    ray.init()\n",
    "except:\n",
    "    print(\"ray already started\")\n",
    "\n",
    "# viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "# feature selection / preprocessing\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold, StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "from augmentdatalib_source.knnor.data_augment import KNNOR\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from feature_selection import FeatureSelector\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from lineartree import LinearTreeClassifier\n",
    "from xgboost import XGBClassifier, DMatrix\n",
    "from catboost import CatBoostClassifier\n",
    "from parallel_train import Trainer\n",
    "from tuning import Tuner\n",
    "\n",
    "# analysis\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_columns\", None) # show all cols\n",
    "\n",
    "# reload modules in py files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weight_to_int(row):\n",
    "    try:\n",
    "        return int(row)\n",
    "    except:\n",
    "        return int(row.split(\" \")[0])\n",
    "\n",
    "\n",
    "def height_to_inches(row):\n",
    "    if type(row) == float:\n",
    "        # 6.10 -> 72 inches\n",
    "        feet, inches = str(row).split(\".\")\n",
    "        return int(feet) * 12 + int(inches)\n",
    "\n",
    "    if type(row) == str:\n",
    "        # 6' 10\" -> 72 inches\n",
    "        feet, inches = str(row).split(\" \")\n",
    "        feet = feet.replace(\"'\", '')\n",
    "        inches = inches.replace('\"', '')\n",
    "        return int(feet) * 12 + int(inches)\n",
    "\n",
    "    if type(row) == int:\n",
    "        return row * 12\n",
    "\n",
    "\n",
    "players_df = pd.read_excel(\"Brdi_db_march.xlsx\", engine=\"openpyxl\").drop(columns=[123, \"id\", \"Data Initials\", \"Code Name\", \"draft status\", ])\n",
    "\n",
    "# if no prev concussions \"# of concussions\" = 0\n",
    "players_df.loc[players_df[\"previous concussions?\"] == \"NO\", '# of concussions'] = 0\n",
    "\n",
    "# \"previous concussions?\" YES/NO -> 0/1\n",
    "players_df[\"previous concussions?\"] = players_df[\"previous concussions?\"].apply(lambda x: 1 if x==\"YES\" else 0)\n",
    "\n",
    "# weight -> int\n",
    "players_df[\"weight\"] = players_df[\"weight\"].apply(weight_to_int)\n",
    "\n",
    "# height -> inches as int\n",
    "players_df[\"height\"] = players_df[\"height\"].apply(height_to_inches)\n",
    "\n",
    "# draft year -> int *not drafted == -1*\n",
    "players_df[\"draft year\"] = players_df[\"draft year\"].apply(lambda x: int(x) if pd.notnull(x) and x != 0 else -1)\n",
    "\n",
    "# draft number -> int *not drafted == -1*\n",
    "players_df[\"draft number\"] = players_df[\"draft number\"].apply(lambda x: int(x) if pd.notnull(x) and x != 0 else -1)\n",
    "\n",
    "# create drafted row\n",
    "players_df[\"drafted\"] = players_df[\"draft number\"].apply(lambda x: 0 if x == -1 else 1)\n",
    "column_to_move = players_df.pop(\"drafted\")\n",
    "players_df.insert(8, \"drafted\", column_to_move)\n",
    "\n",
    "players_df.to_excel(\"Brdi_db_clean.xlsx\")\n",
    "players_df.head(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autopct_format(values):\n",
    "        def my_format(pct):\n",
    "            total = sum(values)\n",
    "            val = int(round(pct*total/100.0))\n",
    "            return '{:.1f}%\\n({v:d})'.format(pct, v=val)\n",
    "        return my_format\n",
    "\n",
    "positions = players_df.groupby(\"Position\").year.count()\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "plt.title(label=\"Distribution of Positions\")\n",
    "plt.pie(positions, labels = positions.index, colors = colors, autopct=autopct_format(positions))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = players_df.groupby(\"age as of June 1\").year.count()\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "plt.title(label=\"Distribution of Ages\")\n",
    "plt.pie(ages, labels = ages.index, colors = colors, autopct=autopct_format(ages))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concussions = players_df.groupby(\"previous concussions?\").year.count()\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "plt.title(label=\"Distribution of Concussions\")\n",
    "plt.pie(concussions, labels = [\"No\", \"Yes\"], colors = colors, autopct=autopct_format(concussions))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drafted = players_df.groupby(\"draft year\").year.count()\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "plt.title(label=\"Distribution of Played in NHL\")\n",
    "plt.pie(drafted, labels = drafted.index, colors = colors, autopct=autopct_format(drafted))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seaborn_conf_matrix(cm):\n",
    "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted SVM for Draft Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of scaling\n",
    "\n",
    "standard: $\\frac{x_k(d) - \\mu_k}{ \\sigma_k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred):\n",
    "    otuput = f\"\"\"precision: {precision_score(y_test, y_pred)}\\nrecall: {recall_score(y_test, y_pred)}\\naccuracy: {accuracy_score(y_test, y_pred)}\\nf1: {f1_score(y_test, y_pred)}\"\"\"\n",
    "    print(otuput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_test, y_pred):\n",
    "    otuput = f\"\"\"precision: {precision_score(y_test, y_pred)}\\nrecall: {recall_score(y_test, y_pred)}\\naccuracy: {accuracy_score(y_test, y_pred)}\\nf1: {f1_score(y_test, y_pred)}\"\"\"\n",
    "    print(otuput)\n",
    "\n",
    "def print_all_metrics(y_test, yhat, classifier=True):\n",
    "    print(f'\\n\\n-----MODULE {\"CLASSIFICATION\" if classifier else \"PREDICTION\"} METRICS-----')\n",
    "    if classifier:    \n",
    "        print_metrics(y_test, yhat)\n",
    "        seaborn_conf_matrix(confusion_matrix(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, target_col=\"NHL\"):\n",
    "\n",
    "    non_feature_cols = [\"year\",\"DOB\", \"draft year\", \"shoots\", \"Position\", \"drafted\", \"draft number\"]\n",
    "\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = scale_and_split(df, scaler=\"None\", test_size=.3, target_col=\"NHL\")\n",
    "    # scale X train\n",
    "    y = df[target_col]\n",
    "    X = df.drop(columns=non_feature_cols + [target_col])\n",
    "\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_split(df, scaler=\"standard\", target_col=\"drafted\", test_size=0.2, shuffle=False, print_columns=False, return_feature_names=False):\n",
    "    non_feature_cols = [\"year\",\"DOB\", \"draft year\", \"shoots\", \"Position\"]\n",
    "\n",
    "    # if target == \"drafted\":\n",
    "    #     df.drop(df.loc[df['line_race']==0].index, inplace=True)\n",
    "    #     target = df[target_col]\n",
    "    #     features = df.drop(columns=non_feature_cols + [target_col])\n",
    "        \n",
    "    X, y = create_dataset(df, target_col=target_col)\n",
    "    feature_names = X.columns\n",
    "    if print_columns:\n",
    "        print(X.columns)\n",
    "        \n",
    "   \n",
    "\n",
    "    if scaler == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    # shuffler = np.random.permutation(len(X))\n",
    "    # X = X[shuffler]\n",
    "    # y = y[shuffler]\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=np.random.randint(25) if shuffle else 50)\n",
    "         \n",
    "    if not return_feature_names:\n",
    "        return  X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "       return  X_train, X_test, y_train, y_test, feature_names\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['age as of June 1', 'height', 'weight', 'NHL',\n",
    "       'previous concussions?', '# of concussions', 'bimanual score: washer',\n",
    "       'Bimanual Score: Button', 'RT_V', 'RT_HR', 'Delta_RT', 'MT_V', 'MT_HR',\n",
    "       'Delta_MT', 'TMT_V', 'TMT_HR', 'CMT: V', 'CMT: HR', 'cvRT_V', 'cvRT_HR',\n",
    "       'stdRT_V', 'stdRT_HR', 'Ball Path_V', 'Ball Path_HR', 'Delta_BallPath',\n",
    "       'FullPath_V', 'FullPath_HR', 'Delta_Fullpath', 'Corrective_V',\n",
    "       'Corrective_HR', 'PeakV_V', 'PeakV_HR', 'Delta_PV', 'AE_V', 'AE_HR',\n",
    "       'Delta_AE', 'VE_V', 'VE_HR', 'Delta: VE', 'AbsOnAxis_V', 'AbsOnAxis_HR',\n",
    "       'Delta_OnAxis', 'AbsOffAxis_V', 'AbsOffAxis_HR', 'Delta_OffAxis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, \"standard\", test_size=.2)\n",
    "\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "seaborn_conf_matrix(confusion_matrix(y_test, yhat))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from lineartree import LinearTreeRegressor\n",
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, \"standard\", test_size=.2)\n",
    "model = LinearTreeClassifier(base_estimator=LogisticRegression())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "display(model.plot_model(feature_names=feature_cols))\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "seaborn_conf_matrix(confusion_matrix(y_test, yhat))\n",
    "print_metrics(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, \"standard\", test_size=.2)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "yhat = clf.predict(X_test)\n",
    "\n",
    "tree.plot_tree(clf, \n",
    "                   feature_names=feature_cols,  \n",
    "                   class_names=[\"drafted\", \"not drafted\"],\n",
    "                   filled=True)\n",
    "\n",
    "print_metrics(y_test, yhat)\n",
    "print(yhat)\n",
    "print(list(y_test))\n",
    "plt.show()\n",
    "seaborn_conf_matrix(confusion_matrix(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralClassifier:\n",
    "    def __init__(self, base_classifier=LogisticRegression(), data=None, target=\"NHL\") -> None:\n",
    "        self.df = data\n",
    "        self.target = target\n",
    "        self.features = []\n",
    "\n",
    "        self.model = base_classifier\n",
    "    \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [],[],[],[],\n",
    "        self.y_pred = []\n",
    "\n",
    "        self.accuracy_metrics = {\"precision\" : 0, \"recall\" : 0, \"f1\" : 0, \"accuracy\" : 0 }\n",
    "\n",
    "    def train_test_split(self, scaler=\"standard\", test_size=0.2, shuffle=False):\n",
    "            non_feature_cols = [\"year\",\"DOB\", \"draft year\", \"shoots\", \"Position\"]\n",
    "\n",
    "            # if target == \"drafted\":\n",
    "            #     df.drop(df.loc[df['line_race']==0].index, inplace=True)\n",
    "            #     target = df[target_col]\n",
    "            #     features = df.drop(columns=non_feature_cols + [target_col])\n",
    "                \n",
    "\n",
    "            target_col=self.target\n",
    "\n",
    "            y = self.df[target_col]\n",
    "            \n",
    "            if target_col == \"drafted\":\n",
    "                X = self.df.drop(columns=non_feature_cols + [target_col] + [\"draft number\", \"# of concussions\", \"previous concussions?\"])\n",
    "            else:\n",
    "                X = self.df.drop(columns=non_feature_cols + [target_col])\n",
    "\n",
    "            self.features = X.columns\n",
    "            X = X.fillna(X.mean())\n",
    "\n",
    "            if scaler == \"standard\":\n",
    "                scaler = StandardScaler()\n",
    "            \n",
    "            # shuffler = np.random.permutation(len(X))\n",
    "            # X = X[shuffler]\n",
    "            # y = y[shuffler]\n",
    "\n",
    "            X = scaler.fit_transform(X)\n",
    "\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, stratify=y, random_state=np.random.randint(25) if shuffle else 50)\n",
    "\n",
    "            return self\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        return self\n",
    "\n",
    "    def predict(self,):\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        return self\n",
    "\n",
    "    def get_accuracy_metrics(self):\n",
    "        self.accuracy_metrics[\"precision\"] = precision_score(self.y_test, self.y_pred)\n",
    "        self.accuracy_metrics[\"recall\"] = recall_score(self.y_test, self.y_pred)\n",
    "        self.accuracy_metrics[\"f1\"] = f1_score(self.y_test, self.y_pred)\n",
    "        self.accuracy_metrics[\"accuracy\"] = accuracy_score(self.y_test, self.y_pred)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def accuracy_heatmap(self):\n",
    "        cm = confusion_matrix(self.y_test, self.y_pred)\n",
    "        group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "        group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "        labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "        labels = np.asarray(labels).reshape(2,2)\n",
    "        sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "        plt.show()\n",
    "        return self\n",
    "\n",
    "    def display_metrics(self):\n",
    "        output = f\"\"\"----- Classifier: {type(self.model).__name__}-----\\n  * precision: {self.accuracy_metrics[\"precision\"]}\\n  * recall: {self.accuracy_metrics[\"recall\"]}\\n  * f1: {self.accuracy_metrics[\"f1\"]}\\n  * accuracy: {self.accuracy_metrics[\"accuracy\"]}\"\"\"\n",
    "        print(output)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Can categorize athletes who actually played on field based on their performance metrics: Binary Classification where the target is to predict if played NHL or not (column J) using performance metrics as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_in_nhl = players_df.groupby(\"NHL\").year.count()\n",
    "colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "plt.title(label=\"Distribution of Played in NHL\")\n",
    "plt.pie(played_in_nhl, labels = [\"No\", \"Yes\"], colors = colors, autopct=autopct_format(played_in_nhl))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GeneralClassifier(base_classifier=LogisticRegression(), data=players_df, target=\"NHL\")\n",
    "clf = clf.train_test_split().fit().predict().get_accuracy_metrics()\n",
    "\n",
    "clf.display_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"standard\", test_size=.2, target_col=\"NHL\")\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "print_all_metrics(y_test, yhat, classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"standard\", test_size=.2, target_col=\"NHL\")\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "print_all_metrics(y_test, yhat, classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"standard\", test_size=.2, target_col=\"NHL\")\n",
    "model = MLPClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "print_all_metrics(y_test, yhat, classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"standard\", test_size=.2, target_col=\"NHL\")\n",
    "model = SVC()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "print_all_metrics(y_test, yhat, classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"standard\", test_size=.2, target_col=\"NHL\")\n",
    "model = GaussianNB()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)\n",
    "\n",
    "\n",
    "print_all_metrics(y_test, yhat, classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"None\", test_size=.3, target_col=\"NHL\")\n",
    "\n",
    "model = XGBClassifier(base_score=0.6, booster='gbtree', max_depth=10, n_estimators=200) \n",
    "\n",
    "# input matrix form for XGBoost\n",
    "data_matrix = DMatrix(data=X_train, label=y_train)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "print(\"Mean cross-validation score: %.2f\" % scores.mean())\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    " \n",
    "print_all_metrics(y_test, y_pred, classifier=True)\n",
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "# kf_cv_scores = cross_val_score(model, X_train, y_train, cv=kfold )\n",
    "# print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n",
    "\n",
    "# print_all_metrics(y_test, yhat, classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = scale_and_split(players_df, scaler=\"None\", test_size=.3, target_col=\"NHL\")\n",
    "\n",
    "model = CatBoostClassifier()\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False, )\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print_all_metrics(y_test, y_pred, classifier=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "DF to keep track off all the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Out of the box classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers = [LogisticRegression(), LinearTreeClassifier(base_estimator=LogisticRegression()), DecisionTreeClassifier(), MLPClassifier(), SVC(),GaussianNB(), XGBClassifier(), CatBoostClassifier(), ]\n",
    "classifiers = [LogisticRegression(), MLPClassifier(), SVC(), CatBoostClassifier()]\n",
    "all_f1_results = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0)\n",
    "\n",
    "X, y = create_dataset(players_df)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "f1_scores = []\n",
    "df_f1_scores = []\n",
    "df_models = []\n",
    "df_precisions = []\n",
    "df_recalls = []\n",
    "df_accuracys = []\n",
    "\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf = clone(clf)\n",
    "    indiv_f1_scores = []\n",
    "    indiv_df_scores = []\n",
    "    indiv_df_models = []\n",
    "    indiv_df_precisions = []\n",
    "    indiv_df_recalls = []\n",
    "    indiv_df_accuracys = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        \n",
    "\n",
    "        X_train = X[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        X_test= X[test_index]\n",
    "        y_test = y.iloc[test_index]    \n",
    "\n",
    "\n",
    "        model_name = clf.__class__.__name__\n",
    "        if  model_name == \"CatBoostClassifier\": \n",
    "            clf.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        indiv_f1_scores.append(f1)\n",
    "        indiv_df_precisions.append(precision_score(y_test, y_pred))\n",
    "        indiv_df_recalls.append(recall_score(y_test, y_pred))\n",
    "        indiv_df_accuracys.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "\n",
    "    f1_scores.append({model_name : np.mean(indiv_f1_scores)})\n",
    "    df_f1_scores.append(np.mean(indiv_f1_scores))\n",
    "    df_precisions.append(np.mean(indiv_df_precisions))\n",
    "    df_recalls.append(np.mean(indiv_df_recalls))\n",
    "    df_models.append(model_name)\n",
    "    df_accuracys.append(np.mean(indiv_df_accuracys))\n",
    "\n",
    "results_otb = pd.DataFrame({\"Model\" : df_models, \"Precision\" : df_precisions, \"Recall\" : df_recalls, \"Accuracy\" : df_accuracys, \"F1\" : df_f1_scores}).set_index(\"Model\").sort_values(by=\"F1\", ascending=False).round(3)\n",
    "all_f1_results['f1_otb'] = df_f1_scores\n",
    "all_f1_results.index = df_models\n",
    "all_f1_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Class Imbalance\n",
    "\n",
    "1. SMOTE\n",
    "2. ADASyn\n",
    "2. KNN OveRsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X, y, spl_type=\"SMOTE\"):\n",
    "    techniques = {\n",
    "        \"SMOTE\" : SMOTE(random_state=0),\n",
    "        \"ADASYN\" : ADASYN(random_state=0),\n",
    "        \"RANDOM\" : RandomOverSampler(random_state=0),\n",
    "    }\n",
    "\n",
    "    sampler = techniques[spl_type]\n",
    "\n",
    "    X, y = sampler.fit_resample(X, y,)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0)\n",
    "\n",
    "\n",
    "X, y = create_dataset(players_df, target_col=\"NHL\")\n",
    "\n",
    "# resample\n",
    "X, y = balance_dataset(X, y, spl_type=\"SMOTE\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "f1_scores = []\n",
    "df_f1_scores = []\n",
    "df_models = []\n",
    "df_precisions = []\n",
    "df_recalls = []\n",
    "df_accuracys = []\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "for clf in classifiers:\n",
    "    clf = clone(clf)\n",
    "    indiv_f1_scores = []\n",
    "    indiv_df_scores = []\n",
    "    indiv_df_models = []\n",
    "    indiv_df_precisions = []\n",
    "    indiv_df_recalls = []\n",
    "    indiv_df_accuracys = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        \n",
    "\n",
    "        X_train = X[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        X_test= X[test_index]\n",
    "        y_test = y.iloc[test_index]    \n",
    "\n",
    "\n",
    "        model_name = clf.__class__.__name__\n",
    "        if  model_name == \"CatBoostClassifier\": \n",
    "            clf.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        indiv_f1_scores.append(f1)\n",
    "        indiv_df_precisions.append(precision_score(y_test, y_pred))\n",
    "        indiv_df_recalls.append(recall_score(y_test, y_pred))\n",
    "        indiv_df_accuracys.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "\n",
    "    f1_scores.append({model_name : np.mean(indiv_f1_scores)})\n",
    "    df_f1_scores.append(np.mean(indiv_f1_scores))\n",
    "    df_precisions.append(np.mean(indiv_df_precisions))\n",
    "    df_recalls.append(np.mean(indiv_df_recalls))\n",
    "    df_models.append(model_name)\n",
    "    df_accuracys.append(np.mean(indiv_df_accuracys))\n",
    "\n",
    "results_balance = pd.DataFrame({\"Model\" : df_models, \"Precision\" : df_precisions, \"Recall\" : df_recalls, \"Accuracy\" : df_accuracys, \"F1\" : df_f1_scores}).set_index(\"Model\").sort_values(by=\"F1\", ascending=False).round(3)\n",
    "all_f1_results['f1_balanced'] = df_f1_scores\n",
    "all_f1_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Balancing does not seem to help the accuracy of the models at all. I do not understand the reason for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop correlated features using Pearson Correlation\n",
    "\n",
    "Features that are very highly correlated with one another don't all need to be in the dataset. Too many features encourages the curse of dimensionality and so if we can be smart at reducing features, we may serve to gain model accuracy as the feature space decreases. \n",
    "\n",
    "<br>\n",
    "Note:<br> \n",
    "\n",
    "- Chi square didn't work becuase values were negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(players_df, target_col=\"NHL\")\n",
    "cor = pd.DataFrame(X).corr()\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(cor, cmap=plt.cm.CMRmap_r,annot=True)\n",
    "plt.show()  \n",
    "\n",
    "def correlation(dataset, threshold):\n",
    "\n",
    "    \"\"\"\n",
    "    Find all pairs of collumns with correllation > .7. Add one of the pairs to a set to be dropped\n",
    "    \"\"\"\n",
    "    col_corr = set()  \n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: \n",
    "                colname = corr_matrix.columns[i]                  \n",
    "                col_corr.add(colname)\n",
    "    return col_corr  \n",
    "\n",
    "\n",
    "corr_features = correlation(X, .7)\n",
    "corr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0)\n",
    "\n",
    "X, y = create_dataset(players_df)\n",
    "\n",
    "\n",
    "# drop correlated features\n",
    "corr_features = correlation(X, .7)\n",
    "X.drop(corr_features, axis=1)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "f1_scores = []\n",
    "df_f1_scores = []\n",
    "df_models = []\n",
    "df_precisions = []\n",
    "df_recalls = []\n",
    "df_accuracys = []\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "for clf in classifiers:\n",
    "    clf = clone(clf)\n",
    "    indiv_f1_scores = []\n",
    "    indiv_df_scores = []\n",
    "    indiv_df_models = []\n",
    "    indiv_df_precisions = []\n",
    "    indiv_df_recalls = []\n",
    "    indiv_df_accuracys = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        \n",
    "\n",
    "        X_train = X[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        X_test= X[test_index]\n",
    "        y_test = y.iloc[test_index]    \n",
    "\n",
    "\n",
    "        model_name = clf.__class__.__name__\n",
    "        if  model_name == \"CatBoostClassifier\": \n",
    "            clf.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        indiv_f1_scores.append(f1)\n",
    "        indiv_df_precisions.append(precision_score(y_test, y_pred))\n",
    "        indiv_df_recalls.append(recall_score(y_test, y_pred))\n",
    "        indiv_df_accuracys.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "\n",
    "    f1_scores.append({model_name : np.mean(indiv_f1_scores)})\n",
    "    df_f1_scores.append(np.mean(indiv_f1_scores))\n",
    "    df_precisions.append(np.mean(indiv_df_precisions))\n",
    "    df_recalls.append(np.mean(indiv_df_recalls))\n",
    "    df_models.append(model_name)\n",
    "    df_accuracys.append(np.mean(indiv_df_accuracys))\n",
    "\n",
    "results_feat_sel = pd.DataFrame({\"Model\" : df_models, \"Precision\" : df_precisions, \"Recall\" : df_recalls, \"Accuracy\" : df_accuracys, \"F1\" : df_f1_scores}).set_index(\"Model\").sort_values(by=\"F1\", ascending=False).round(3)\n",
    "all_f1_results['f1_rm_corr_features'] = df_f1_scores\n",
    "all_f1_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Columns based on ${Chi}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_p_values=chi2(X,y)\n",
    "p_values=pd.Series(f_p_values[1])\n",
    "p_values.index=X_train.columns\n",
    "p_values = p_values.sort_index(ascending=False)\n",
    "p_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping only the top N=8 features using extra tree classifier\n",
    "\n",
    "Notes: <br>\n",
    "\n",
    "* Trains differently every time and I am not sure why. Might be because of the ExtraTreesclassifier although that should be seeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "def get_n_important_features(X, y, n_features=10):\n",
    "    model = ExtraTreesClassifier(random_state=0)\n",
    "    model.fit(X, y)\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=X.columns).nlargest(n_features)\n",
    "    \n",
    "    top_n_columns=feat_importances.keys().to_list()\n",
    "\n",
    "    top_n_features = pd.DataFrame({'importance' : feat_importances}, index=top_n_columns).sort_values(by=\"importance\", ascending=False)\n",
    "    return top_n_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0)\n",
    "\n",
    "X, y = create_dataset(players_df)\n",
    "\n",
    "important_features = get_n_important_features(X, y, 10)\n",
    "\n",
    "# drop all but 10 most important features\n",
    "X = X.drop(list(set(X.columns) - set(important_features.index)), axis=1)\n",
    "print(X.columns)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "f1_scores = []\n",
    "df_f1_scores = []\n",
    "df_models = []\n",
    "df_precisions = []\n",
    "df_recalls = []\n",
    "df_accuracys = []\n",
    "\n",
    "for clf in classifiers:\n",
    "    clf = clone(clf)\n",
    "    indiv_f1_scores = []\n",
    "    indiv_df_scores = []\n",
    "    indiv_df_models = []\n",
    "    indiv_df_precisions = []\n",
    "    indiv_df_recalls = []\n",
    "    indiv_df_accuracys = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X, y)):\n",
    "        \n",
    "\n",
    "        X_train = X[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "\n",
    "        X_test= X[test_index]\n",
    "        y_test = y.iloc[test_index]    \n",
    "\n",
    "\n",
    "        model_name = clf.__class__.__name__\n",
    "        if  model_name == \"CatBoostClassifier\": \n",
    "            clf.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=False)\n",
    "        else:\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "        indiv_f1_scores.append(f1)\n",
    "        indiv_df_precisions.append(precision_score(y_test, y_pred))\n",
    "        indiv_df_recalls.append(recall_score(y_test, y_pred))\n",
    "        indiv_df_accuracys.append(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "\n",
    "\n",
    "    f1_scores.append({model_name : np.mean(indiv_f1_scores)})\n",
    "    df_f1_scores.append(np.mean(indiv_f1_scores))\n",
    "    df_precisions.append(np.mean(indiv_df_precisions))\n",
    "    df_recalls.append(np.mean(indiv_df_recalls))\n",
    "    df_models.append(model_name)\n",
    "    df_accuracys.append(np.mean(indiv_df_accuracys))\n",
    "\n",
    "results_feat_sel = pd.DataFrame({\"Model\" : df_models, \"Precision\" : df_precisions, \"Recall\" : df_recalls, \"Accuracy\" : df_accuracys, \"F1\" : df_f1_scores}).set_index(\"Model\").sort_values(by=\"F1\", ascending=False).round(3)\n",
    "all_f1_results['f1_top_n_features'] = df_f1_scores\n",
    "all_f1_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Feature selection via Forward selection.\n",
    "\n",
    "This was used in the SOA draft by the numbers and is a brute force technique for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_num_to_name(indicies, df):\n",
    "    cols = df.columns\n",
    "    return list(map(lambda i: cols[i], indicies))\n",
    "\n",
    "feature_num_to_name([0, 3, 4, 7, 8, 9, 11, 30], X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def do_sequential_selection(estimator, df, forward=True, floating=True, scoring=\"f1\"):\n",
    "\n",
    "    X, y = create_dataset(df)\n",
    "    # print(X.columns)\n",
    "\n",
    "    X_train, X_test, y_train, y_test= train_test_split(X, y, \n",
    "                                                    stratify=y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=1)\n",
    "                                                    \n",
    "    sfs1 = SequentialFeatureSelector(estimator=estimator, \n",
    "            k_features=(7, 15),\n",
    "            forward=forward, \n",
    "            floating=floating, \n",
    "            scoring=scoring,\n",
    "            cv=5)\n",
    "\n",
    "    pipe = make_pipeline(StandardScaler(), sfs1)\n",
    "\n",
    "    pipe.fit(X_train, y_train,)\n",
    "\n",
    "    print('best combination (ACC: %.3f): %s\\n' % (sfs1.k_score_, feature_num_to_name(sfs1.k_feature_idx_, X_train)), end=\"\\n\\n\")\n",
    "    print('all subsets:\\n', sfs1.subsets_)\n",
    "    plot_sfs(sfs1.get_metric_dict(), kind='std_err')\n",
    "\n",
    "    return sfs1\n",
    "\n",
    "\n",
    "floating, forward = [\"True\"] * 2\n",
    "scoring=\"f1\"\n",
    "estimator=MLPClassifier()\n",
    "sfs1 = do_sequential_selection(estimator=estimator, df=players_df, floating=floating, forward=forward, scoring=scoring), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0).split(X, y)\n",
    "list(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from feature_selection import FeatureSelector\n",
    "from data_cleaning import clean_raw_data, create_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "\n",
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0).split(X, y))\n",
    "\n",
    "ESTIMATOR = DecisionTreeClassifier()\n",
    "ftsl = FeatureSelector(ESTIMATOR, selection_type=\"forward\", floating=True, scoring=\"f1\", k_features=2, cv=kf)\n",
    "ftsl.fit(X, y)\n",
    "\n",
    "ftsl.get_results()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "X = X[[\"previous concussions?\", \"DR Errors: V\", \"TMT_V\", \"cvRT_HR\", \"PeakV_V\", \"Delta: VE\", \"AbsOffAxis_V\", \"age as of June 1\"]]\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, \n",
    "                                                    stratify=y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=1)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), MLPClassifier())\n",
    "pipe.fit(X, y)                                        \n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsl.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame()\n",
    "\n",
    "# test.append(ftsl.get_results())\n",
    "\n",
    "test = pd.concat([test,ftsl.get_results()] )\n",
    "test = pd.concat([test,ftsl.get_results()] )\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0).split(X, y))\n",
    "\n",
    "@ray.remote\n",
    "def parrallell_feature_selection(estimator):\n",
    "    # ESTIMATOR = MLPClassifier(max_iter=300)\n",
    "    ftsl = FeatureSelector(estimator, selection_type=\"backward\", floating=True, scoring=\"f1\", k_features=25, cv=kf)\n",
    "    ftsl.fit(X, y)\n",
    "    results = ftsl.get_results()\n",
    "    return results\n",
    "\n",
    "\n",
    "all_parallel_results = pd.DataFrame()\n",
    "classifiers = [DecisionTreeClassifier(), RandomForestClassifier(), SVC()]\n",
    "# classifiers = [LogisticRegression(), MLPClassifier(), XGBClassifier()]\n",
    "result_ids = []\n",
    "for cls in classifiers:\n",
    "    print(type(cls).__name__)\n",
    "    result_ids.append(parrallell_feature_selection.remote(cls))\n",
    "\n",
    "for r in ray.get(result_ids):\n",
    "    res_df = r\n",
    "    all_parallel_results = pd.concat([all_parallel_results, res_df])\n",
    "\n",
    "\n",
    "all_parallel_results.sort_values(by=\"score\", ascending=False).to_excel(f\"./training_output/DT_RF_SVC_25_backward_floating_f1{uuid.uuid4()}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do feature selection on 3 different models\n",
    "\n",
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=5, random_state=0).split(X, y))\n",
    "models = [LogisticRegression(), MLPClassifier(), XGBClassifier()]\n",
    "kwargs = {\"selection_type\":\"forward\", \"floating\":True, \"scoring\":\"f1\", \"k_features\": 2, \"cv\":kf}\n",
    "\n",
    "trainer = Trainer(X, y, models)\n",
    "trainer.train(how=\"feature_selection\", kwargs=kwargs)\n",
    "\n",
    "output = trainer.get_results(filename=\"LR_MLP_XGB_All\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_val_score(model, kf, features):\n",
    "    df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "\n",
    "    X, y = create_dataset(df, target_col=\"NHL\")\n",
    "\n",
    "    X = X[features]\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        pipe = make_pipeline(StandardScaler(), model)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred = pipe.predict(X_test)\n",
    "        scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    return np.mean(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "\n",
    "# from feature selectoin DT_RF_SVC_25Backwardf_floating_f1721...\n",
    "features = ['age as of June 1', 'height', 'weight', 'previous concussions?', '# of concussions', 'Bimanual Score: Button', 'DR Errors: V', 'DR Errors: HR', 'RT_HR', 'MT_HR', 'Delta_MT', 'TMT_V', 'TMT_HR', 'Ball Path_V', 'Delta_BallPath', 'FullPath_V', 'FullPath_HR', 'Delta_PV', 'VE_V', 'Delta: VE', 'AbsOffAxis_HR', 'Delta_OffAxis']\n",
    "\n",
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=3, random_state=0).split(X, y))\n",
    "\n",
    "\n",
    "# models = [LogisticRegression(), MLPClassifier(), XGBClassifier()]\n",
    "\n",
    "\n",
    "print(\"f1 score: \", get_cross_val_score(DecisionTreeClassifier(), kf, features=features))\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "hyperparams = {\n",
    "    \"criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"max_depth\" : range(1, 12),\n",
    "    \"min_samples_split\" : range(1, 12),\n",
    "    \"min_samples_leaf\" : range(1, 12),\n",
    "    \"max_features\" : [\"sqrt\", \"log2\", None],\n",
    "    \"class_weight\" : [None, \"balanced\"],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "model_tuner = Tuner(model, hyperparams, cv=kf)\n",
    "model_tuner.tune(X, y)\n",
    "\n",
    "best_params = model_tuner.get_best_params()\n",
    "best_estimator = model_tuner.get_best_estimator()\n",
    "results = model_tuner.get_results()\n",
    "\n",
    "print(\"best after model tuning\")\n",
    "print(model_tuner.get_best_estimator(), model_tuner.get_best_score(), model_tuner.get_best_params())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this takes 106 mins to run as it is right now\n",
    "\n",
    "\n",
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "\n",
    "# best Decision Tree features from feature selectoin DT_RF_SVC_25Backwardf_floating_f1721...\n",
    "features = ['age as of June 1', 'height', 'weight', 'previous concussions?', '# of concussions', 'Bimanual Score: Button', 'DR Errors: V', 'DR Errors: HR', 'RT_HR', 'MT_HR', 'Delta_MT', 'TMT_V', 'TMT_HR', 'Ball Path_V', 'Delta_BallPath', 'FullPath_V', 'FullPath_HR', 'Delta_PV', 'VE_V', 'Delta: VE', 'AbsOffAxis_HR', 'Delta_OffAxis']\n",
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=3, random_state=0).split(X, y))\n",
    "\n",
    "\n",
    "# models = [LogisticRegression(), MLPClassifier(), XGBClassifier()]\n",
    "\n",
    "\n",
    "print(\"f1 score: \", get_cross_val_score(RandomForestClassifier(), kf, features=features))\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "hyperparams = {\n",
    "    \"bootstrap\" : [True, False],\n",
    "    \"max_depth\" : range(1, 12),\n",
    "    \"min_samples_split\" : range(1, 12),\n",
    "    \"min_samples_leaf\" : range(1, 12),\n",
    "    \"max_features\" : [\"sqrt\", \"log2\", None],\n",
    "    \"class_weight\" : [None, \"balanced\"],\n",
    "    \"n_estimators\" : np.linspace(10, 100, 10, dtype=int),\n",
    "}\n",
    "\n",
    "\n",
    "model_tuner = Tuner(model, hyperparams, cv=kf)\n",
    "model_tuner.tune(X, y)\n",
    "\n",
    "best_params = model_tuner.get_best_params()\n",
    "best_estimator = model_tuner.get_best_estimator()\n",
    "results = model_tuner.get_results()\n",
    "\n",
    "print(\"best after model tuning\")\n",
    "print(model_tuner.get_best_estimator(), model_tuner.get_best_score(), model_tuner.get_best_params())\n",
    "\n",
    "# saving output\n",
    "# f1 score:  0.6868236900494965\n",
    "# Fitting 3 folds for each of 159720 candidates, totalling 479160 fits\n",
    "# best after model tuning\n",
    "# RandomForestClassifier(class_weight='balanced', max_depth=3, max_features=None,\n",
    "#                        min_samples_leaf=8, min_samples_split=6,\n",
    "#                        n_estimators=40) 0.7733333333333334 {'RandomForestClassifier': {'bootstrap': True, 'class_weight': 'balanced', 'max_depth': 3, 'max_features': None, 'min_samples_leaf': 8, 'min_samples_split': 6, 'n_estimators': 40}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "\n",
    "# # from feature selectoin DT_RF_SVC_25Backwardf_floating_f1721...\n",
    "# features = ['age as of June 1', 'height', 'weight', 'previous concussions?', '# of concussions', 'Bimanual Score: Button', 'DR Errors: V', 'DR Errors: HR', 'RT_HR', 'MT_HR', 'Delta_MT', 'TMT_V', 'TMT_HR', 'Ball Path_V', 'Delta_BallPath', 'FullPath_V', 'FullPath_HR', 'Delta_PV', 'VE_V', 'Delta: VE', 'AbsOffAxis_HR', 'Delta_OffAxis']\n",
    "# X = X[features]\n",
    "\n",
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=3, random_state=0).split(X, y))\n",
    "\n",
    "\n",
    "# models = [LogisticRegression(), MLPClassifier(), XGBClassifier()]\n",
    "\n",
    "model = LogisticRegression(max_iter=500, solver=\"liblinear\")\n",
    "\n",
    "print(\"f1 score: \", \n",
    "\n",
    "ple_model(clone(model), kf, features=['age as of June 1', 'height', 'weight', 'previous concussions?', '# of concussions', 'Bimanual Score: Button', 'DR Errors: V', 'DR Errors: HR', 'RT_HR', 'MT_HR', 'Delta_MT', 'TMT_V', 'TMT_HR', 'Ball Path_V', 'Delta_BallPath', 'FullPath_V', 'FullPath_HR', 'Delta_PV', 'VE_V', 'Delta: VE', 'AbsOffAxis_HR', 'Delta_OffAxis']))\n",
    "\n",
    "    # \"penalty\" : [\"l1\", \"l2\", \"elasticnet\",],\n",
    "hyperparams = {\n",
    "    \"dual\" : [True, False],  \n",
    "    \"C\" : np.linspace(0.1, 1, 10),\n",
    "    \"class_weight\" : [None, \"balanced\"],\n",
    "    # \"solver\" : [\"liblinear\"]\n",
    "}\n",
    "\n",
    "\n",
    "model_tuner = Tuner(model, hyperparams, cv=kf)\n",
    "model_tuner.tune(X, y)\n",
    "\n",
    "best_params = model_tuner.get_best_params()\n",
    "best_estimator = model_tuner.get_best_estimator()\n",
    "results = model_tuner.get_results()\n",
    "\n",
    "print(\"best after model tuning\")\n",
    "print(model_tuner.get_best_estimator(), model_tuner.get_best_score(), model_tuner.get_best_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selection import FeatureSelector\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "# Do feature selection on 3 different models\n",
    "\n",
    "df = clean_raw_data(filename=\"./Implementation/Brdi_db_march.xlsx\")\n",
    "X, y = create_dataset(df, target_col=\"NHL\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=2, random_state=0).split(X_train, y_train))\n",
    "model = LogisticRegression(solver=\"liblinear\", max_iter=400)\n",
    "kwargs = {\"selection_type\":\"forward\", \"floating\":True, \"scoring\":\"f1\", \"k_features\": (10,20), \"cv\":2}\n",
    "\n",
    "# trainer = Trainer(X, y, models)\n",
    "# self.training_args = dict(kwargs)\n",
    "ftsl = FeatureSelector(model, **kwargs)\n",
    "ftsl = ftsl.fit(X_train, y_train)\n",
    "results = ftsl.get_results()\n",
    "# results\n",
    "\n",
    "plot_sfs(metric_dict=ftsl.selector.get_metric_dict(), kind=\"ci\")\n",
    "# display()\n",
    "\n",
    "# output = trainer.get_results(filename=\"LR_MLP_XGB_All\")\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = list(StratifiedShuffleSplit(test_size=.2, n_splits=3, random_state=0).split(X, y))\n",
    "get_cross_val_score(LogisticRegression, kf, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftsl.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1 = ftsl.selector\n",
    "X_train_sfs = sfs1.transform(X_train)\n",
    "X_test_sfs = sfs1.transform(X_test)\n",
    "\n",
    "# Fit the estimator using the new feature subset\n",
    "# and make a prediction on the test data\n",
    "model.fit(X_train_sfs, y_train)\n",
    "y_pred = model.predict(X_test_sfs)\n",
    "\n",
    "acc = float((y_test == y_pred).sum()) / y_pred.shape[0]\n",
    "print('Test set accuracy: %.2f %%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "import mlxtend\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "lr1 = LogisticRegression()\n",
    "lr2 = LogisticRegression()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "sfs = SFS(estimator=lr1, \n",
    "           k_features=(10,20),\n",
    "           scoring='accuracy',\n",
    "           clone_estimator=False,\n",
    "           cv=2,\n",
    "           n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"sfs\",sfs), (\"lr\", lr2)])\n",
    "\n",
    "param_grid = {'sfs__estimator__C': [0.1, 1.0, 10.0]}\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  n_jobs=-1, \n",
    "                  cv=2, \n",
    "                  verbose=1, \n",
    "                  refit=True)\n",
    "\n",
    "# run gridearch\n",
    "gs = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gs.cv_results_['params'])): \n",
    "    print(gs.cv_results_['params'][i], 'test acc.:', gs.cv_results_['mean_test_score'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs.get_metric_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters via GridSearch\", gs.best_params_)\n",
    "pipe.set_params(**gs.best_params_).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training_results_df = get_all_results(\"./training_output/\")\n",
    "all_training_results_df.to_excel(\"./aggregate_results/aggregate_best_features.xlsx\", index=False)\n",
    "all_training_results_df[all_training_results_df[\"training_args\"].notnull()]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c53bd1381eb6c84fcfea4e1c0eea6b9539f3ac457d183ca9abd93d766138cca8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
