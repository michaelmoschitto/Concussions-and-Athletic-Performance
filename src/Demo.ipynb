{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install any packages here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# utility\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tabulate import tabulate\n",
    "\n",
    "#viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "# parallel\n",
    "import ray\n",
    "\n",
    "try:\n",
    "    ray.shutdown()\n",
    "except:\n",
    "    print(\"ray not started\")\n",
    "\n",
    "# ray.init()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, LeaveOneOut, KFold\n",
    "from scipy.stats import loguniform\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector, ExhaustiveFeatureSelector\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import altair\n",
    "import altair as alt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import copy\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from optuna.integration import OptunaSearchCV\n",
    "# from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, RidgeClassifier, LinearRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from lineartree import LinearTreeClassifier, LinearForestClassifier, LinearBoostClassifier\n",
    "\n",
    "\n",
    "# analysis\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, classification_report\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option(\"display.max_columns\", None) # show all cols\n",
    "\n",
    "# from data_cleaning import clean_raw_data, create_dataset, get_all_results\n",
    "\n",
    "from data_cleaning import clean_raw_data, create_dataset\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from mango import Tuner\n",
    "\n",
    "RESULTS_DIR = \"results/\"\n",
    "DATA_DIR = \"data/\"\n",
    "SEED = 42\n",
    "\n",
    "# reload modules in py files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install linear-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# !pip uninstall --yes mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean dataset using functions in src/data_cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "print(\"No missing values in X: \", (X.isna().sum() == 0).all())\n",
    "print(\"No missing values in y: \", (y.isna().sum() == 0).all())\n",
    "\n",
    "print(X.columns)\n",
    "display(X.head(5),X.shape ,y.head(5), y.shape)\n",
    "\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sampler = RandomUnderSampler(random_state=42, sampling_strategy=\"auto\")\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "\n",
    "print(X_resampled.shape, y_resampled.shape, y_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "\n",
    "1. Take best features from feature selection \n",
    "2. Tune model using best features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class NamedFeatureSelector(object):\n",
    "\n",
    "    def __init__(self, column_names):\n",
    "        all_col_names = ['age', 'DOB_d','weight','bimanual score: washer', \"NHL\",\n",
    "                         'RT_V','RT_HR','Delta_RT','MT_V','MT_HR','Delta_MT','TMT_V','TMT_HR','CMT: V','CMT: HR','cvRT_V','cvRT_HR','stdRT_V','stdRT_HR','Ball Path_V','Ball Path_HR','FullPath_V','FullPath_HR','Delta_Fullpath','Corrective_V','Corrective_HR','PeakV_V','PeakV_HR','Delta_PV',\n",
    "                         'AE_V','AE_HR','Delta_AE','VE_V','VE_HR','AbsOnAxis_HR','Delta_OnAxis','AbsOffAxis_V','AbsOffAxis_HR','Delta_OffAxis', 'AbsOnAxis_V'] \n",
    "\n",
    "        self.column_names = column_names\n",
    "        self.column_idx = [all_col_names.index(col_name) for col_name in column_names]\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, self.column_idx]\n",
    "            return X.to_numpy()\n",
    "        else:\n",
    "            X = X[:, self.column_idx]\n",
    "            return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "features_sel = NamedFeatureSelector([\"age\", 'weight', \"NHL\"])\n",
    "X, y = create_dataset(clean_raw_data(\"Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.1, random_state=1, stratify=y)\n",
    "display(X_train)\n",
    "features_sel.transform(X_train)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale Certain Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X, y_ = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "\n",
    "columns_to_exclude = ['DOB_d', 'age', 'weight', 'bimanualscore_washer', \"NHL\"]\n",
    "cols_to_scale = np.setdiff1d(X.columns, columns_to_exclude).tolist()\n",
    "\n",
    "# Define your column transformer\n",
    "myStandardScaler = ColumnTransformer(\n",
    "    remainder='passthrough',\n",
    "    transformers=[\n",
    "        ('StandardScaler', StandardScaler(), cols_to_scale),\n",
    "    ],\n",
    "    n_jobs=-1,  \n",
    ")\n",
    "\n",
    "myRobustScaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('RobustScaler', RobustScaler(), cols_to_scale),\n",
    "    ],\n",
    "    remainder='passthrough',  # Pass through any columns not specified in transformers\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "myMinMaxScaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('MinMaxScaler', MinMaxScaler(), cols_to_scale),\n",
    "    ],\n",
    "    remainder='passthrough',  # Pass through any columns not specified in transformers\n",
    "    n_jobs=-1,  # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([('scaler', myStandardScaler)])\n",
    "\n",
    "X_transformed = pipeline.fit_transform(X)\n",
    "\n",
    "# Get the names of the columns after scaling\n",
    "# Get the names of the columns after scaling\n",
    "X_scaled = pd.DataFrame(X_transformed, columns=[col.split(\"__\")[1] for col in myStandardScaler.get_feature_names_out()])\n",
    "display(X_scaled.head())\n",
    "print(\"No missing values in X_scaled: \", (X_scaled.isna().sum() == 0).all())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Optuna Distribution Class\n",
    "\n",
    "Optuna requires that each parameter in the param_dist dict is of their distribution type. <br> We define a class that conforms to a distribution type in order to create distributions of objects/classes such as the scaling classes (ColumnTransformer) and undersampler (RandomUnderSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDistribution(optuna.distributions.CategoricalDistribution):\n",
    "    def __init__(self, choices):\n",
    "        self.choices = choices\n",
    "        self.size = len(choices)\n",
    "\n",
    "    def single(self):\n",
    "        return self.choices[0]\n",
    "\n",
    "    def to_external_repr(self, internal_repr):\n",
    "        return self.choices[internal_repr]\n",
    "\n",
    "    def to_internal_repr(self, external_repr):\n",
    "        return self.choices.index(external_repr)\n",
    "\n",
    "# ex\n",
    "# scaler_dist = CustomDistribution([myStandardScaler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction using Principle component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "pca = PCA(n_components=.8, random_state=SEED)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "Xt = pipe.fit_transform(X)\n",
    "plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
    "plt.legend(handles=plot.legend_elements()[0], labels=['No Concussion', 'Concussion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function to get different types of cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_cross_validation(X, y, test_size=.2, n_splits=10, type=\"stratified\", random_state=SEED):\n",
    "    if type == \"stratified\":\n",
    "        return list(StratifiedShuffleSplit(test_size=test_size, n_splits=n_splits, random_state=random_state).split(X, y))\n",
    "    elif type == \"leave_one_out\":\n",
    "        return list(LeaveOneOut().split(X, y))\n",
    "    elif type==\"K-Fold\":\n",
    "        return list(KFold(n_splits=n_splits, random_state=random_state, shuffle=True).split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def seaborn_conf_matrix(cm, model_name, result_dir=\"../figures/confusion_matricies\"):\n",
    "    plt.figure()\n",
    "    plt.title(f\"{model_name} Confusion Matrix\")\n",
    "    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(f\"{result_dir}/{model_name}_conf_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def graph_cv_results(results_df):\n",
    "    plt.figure()\n",
    "    # Plot loss scores using seaborn\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    sns.lineplot(data=results_df, x='params', y='mean_train_score', label='CV Training Score')\n",
    "    sns.lineplot(data=results_df, x='params', y='mean_test_score', label='CV Test Score')\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(ticks=[], labels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def scorer(*args):\n",
    "    print(args)\n",
    "    y_pred = clf.predict(X)\n",
    "    score = f1_score(y, y_pred)\n",
    "#     return score\n",
    "    return 1\n",
    "\n",
    "\n",
    "overfitting_scorer = make_scorer(scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def create_classification_report_DF(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    return report_df\n",
    "\n",
    "\n",
    "create_classification_report_DF(list([0,0,0,0,0,0,0,0]), list([1,1,1,1,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model\n",
    "Note: The decorator \"@ray.remote\" is useful to run different seeds in parallel using the ray python package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "scoring_dict = {\n",
    "\n",
    "            \"F1_unweighted\" : \"f1\",\n",
    "            \"F1_weighted\" : make_scorer(f1_score, average=\"weighted\"),\n",
    "            \"F1_0\" : make_scorer(f1_score, labels=[0], average=None),\n",
    "            \"F1_1\" : make_scorer(f1_score, labels=[1], average=None),\n",
    "\n",
    "            \"Precision_unweighted\" :  make_scorer(precision_score, zero_division=0),\n",
    "            \"Precision_weighted\" : make_scorer(precision_score, zero_division=0, average=\"weighted\"),\n",
    "            \"Precision_0\" : make_scorer(precision_score,labels=[0], zero_division=0),\n",
    "            \"Precision_1\" : make_scorer(precision_score,labels=[1], zero_division=0),\n",
    "\n",
    "            \"Recall_unweighted\" : make_scorer(recall_score, zero_division=0),\n",
    "            \"Recall_weighted\" : make_scorer(recall_score, zero_division=0, average=\"weighted\"),\n",
    "            \"Recall_0\" : make_scorer(recall_score, labels=[0], zero_division=0),\n",
    "            \"Recall_1\" : make_scorer(recall_score, labels=[1], zero_division=0),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import plot_tree\n",
    "import graphviz\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "@ray.remote\n",
    "def tune_model(model, param_dist, features=None, n_iter=200, target_col=None, random_state=SEED, scoring=\"F1_unweighted\", search_type=\"random\"):\n",
    "    model_name = type(model).__name__\n",
    "    if \"Bagging\" in model_name or \"Ada\" in model_name:\n",
    "        model_name = model_name + \"_\" + model.estimator.__class__.__name__\n",
    "\n",
    "    X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=target_col)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=random_state, stratify=y)\n",
    "  \n",
    "    cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=random_state)\n",
    "\n",
    "    pipe = ImbPipeline([\n",
    "            ('scaler', \"passthrough\"),\n",
    "            ('selector', \"passthrough\"),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    print(\"SCORING:  \", scoring)\n",
    "#    setup hyperparameter search using Sklearn random search \n",
    "    if search_type == \"random\":\n",
    "        param_search = RandomizedSearchCV(pipe, \n",
    "                                            param_distributions=param_dist, \n",
    "                                            cv=cv,\n",
    "                                            n_iter=n_iter,\n",
    "                                            random_state=random_state,\n",
    "                                            return_train_score=True,\n",
    "                                            scoring=scoring_dict,\n",
    "                                            n_jobs=-1,\n",
    "                                            refit=scoring)\n",
    "    elif search_type == \"tpe\":\n",
    "        pipe = ImbPipeline([\n",
    "            ('scaler', myStandardScaler),\n",
    "            ('selector', \"passthrough\"),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        param_search = OptunaSearchCV(pipe, \n",
    "                                       param_distributions=param_dist,\n",
    "                                       cv=cv,\n",
    "                                       n_trials=n_iter,\n",
    "                                       random_state=random_state,\n",
    "                                       return_train_score=True,\n",
    "                                       scoring=scoring_dict[scoring],\n",
    "                                       n_jobs=-1,\n",
    "                                       refit=True,\n",
    "#                                        enable_pruning=True,\n",
    "                                       timeout=600\n",
    "                                           \n",
    "        )\n",
    "\n",
    "#   run randomsearch looking for best param configuration on training data\n",
    "    param_search.fit(X_train, y_train)\n",
    "    best_estimator = param_search.best_estimator_\n",
    "#   Once hyperparameter search is complete, predict on validation data using best configuration\n",
    "\n",
    "        \n",
    "#     get training classification report\n",
    "    y_pred_train = best_estimator.predict(X_train)\n",
    "    train_report_df = create_classification_report_DF(y_train, y_pred_train)\n",
    "#     print(f\"Classification Report from training: {tabulate(train_report_df, headers='keys', tablefmt='psql')}\")    \n",
    "    \n",
    "#     get validation classification report\n",
    "    y_pred_val = best_estimator.predict(X_val)\n",
    "\n",
    "    search_type_str = \"Random\" if search_type == 'random' else 'Tree-Parzen Estimator'\n",
    "    print(f\"Search Type: {search_type_str}\")\n",
    "    print(f\"y val : {np.array(y_val)}\")\n",
    "    print(f\"y pred: {y_pred_val}\")\n",
    "    weighted_f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "    print(f\"Weighted F1: {weighted_f1}\")\n",
    "    \n",
    "    val_report_df = create_classification_report_DF(y_val, y_pred_val)\n",
    "#     print(f\"Val classification report: {tabulate(val_report_df, headers='keys', tablefmt='psql')}\")\n",
    "    \n",
    "#     collect estimator, params, and seed\n",
    "    estimator_df = pd.DataFrame([{\n",
    "                            \"Best Estimator\" : param_search.best_estimator_,\n",
    "                            \"Best Params\" : param_search.best_params_, \n",
    "                            \"Best Score\" : param_search.best_score_,\n",
    "                            \"Search Type\" : search_type_str,\n",
    "                            \"Seed\" : random_state,\n",
    "                            \"Train F1 Weighted \" : train_report_df.loc[\"weighted avg\", \"f1-score\"],\n",
    "                            \"Test F1 Weighted \" : val_report_df.loc[\"weighted avg\", \"f1-score\"],\n",
    "                            \"Y Val\" : np.array(y_val),\n",
    "                            \"Y Pred\" : y_pred_val\n",
    "                    }])\n",
    "    \n",
    "    \n",
    "    return estimator_df, train_report_df, val_report_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Model Sequential\n",
    "Exact copy of tune_model, but without ray.remote() decorator, few modifications for mlp_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import plot_tree\n",
    "import graphviz\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def tune_model_sequential(model, param_dist, features=None, n_iter=200, target_col=None, random_state=SEED, scoring=\"F1_unweighted\", search_type=\"random\"):\n",
    "    model_name = type(model).__name__\n",
    "    if \"Bagging\" in model_name or \"Ada\" in model_name:\n",
    "        model_name = model_name + \"_\" + model.estimator.__class__.__name__\n",
    "\n",
    "    X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=target_col)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=random_state, stratify=y)\n",
    "  \n",
    "    cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=random_state)\n",
    "\n",
    "    #     convert to tensors if pytorch model\n",
    "    if \"NeuralNet\" in type(model).__name__:\n",
    "        X_train = torch.FloatTensor(X_train.values).to(torch.float64)\n",
    "        X_val = torch.FloatTensor(X_val.values).to(torch.float64)\n",
    "        y_train = torch.FloatTensor(y_train.values).to(torch.float64)\n",
    "        \n",
    "    pipe = ImbPipeline([\n",
    "            ('scaler', \"passthrough\"),\n",
    "            ('selector', \"passthrough\"),\n",
    "            ('model', model)\n",
    "        ])\n",
    "\n",
    "    print(\"SCORING:  \", scoring)\n",
    "#    setup hyperparameter search using Sklearn random search \n",
    "    if search_type == \"random\":\n",
    "        param_search = RandomizedSearchCV(pipe, \n",
    "                                            param_distributions=param_dist, \n",
    "                                            cv=cv,\n",
    "                                            n_iter=n_iter,\n",
    "                                            random_state=random_state,\n",
    "                                            return_train_score=True,\n",
    "                                            scoring=scoring_dict,\n",
    "                                            n_jobs=1 if \"NeuralNet\" in type(model).__name__ else -1,\n",
    "                                            refit=scoring)\n",
    "    elif search_type == \"tpe\":\n",
    "        pipe = ImbPipeline([\n",
    "            ('scaler', myStandardScaler),\n",
    "            ('selector', \"passthrough\"),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        param_search = OptunaSearchCV(pipe, \n",
    "                                       param_distributions=param_dist,\n",
    "                                       cv=cv,\n",
    "                                       n_trials=n_iter,\n",
    "                                       random_state=random_state,\n",
    "                                       return_train_score=True,\n",
    "                                       scoring=scoring_dict[scoring],\n",
    "                                       n_jobs=1 if \"NeuralNet\" in type(model).__name__ else -1,\n",
    "                                       refit=True,\n",
    "#                                        enable_pruning=True,\n",
    "                                       timeout=600\n",
    "                                           \n",
    "        )\n",
    "\n",
    "#   run randomsearch looking for best param configuration on training data\n",
    "    param_search.fit(X_train, y_train)\n",
    "    best_estimator = param_search.best_estimator_\n",
    "#   Once hyperparameter search is complete, predict on validation data using best configuration\n",
    "\n",
    "    \n",
    "#     get training classification report\n",
    "    y_pred_train = best_estimator.predict(X_train)\n",
    "    train_report_df = create_classification_report_DF(y_train, y_pred_train)\n",
    "#     print(f\"Classification Report from training: {tabulate(train_report_df, headers='keys', tablefmt='psql')}\")    \n",
    "    \n",
    "#     get validation classification report\n",
    "    y_pred_val = best_estimator.predict(X_val)\n",
    "\n",
    "    search_type_str = \"Random\" if search_type == 'random' else 'Tree-Parzen Estimator'\n",
    "    print(f\"Search Type: {search_type_str}\")\n",
    "    print(f\"y val : {np.array(y_val)}\")\n",
    "    print(f\"y pred: {y_pred_val}\")\n",
    "    weighted_f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "    print(f\"Weighted F1: {weighted_f1}\")\n",
    "    \n",
    "    val_report_df = create_classification_report_DF(y_val, y_pred_val)\n",
    "#     print(f\"Val classification report: {tabulate(val_report_df, headers='keys', tablefmt='psql')}\")\n",
    "    \n",
    "#     collect estimator, params, and seed\n",
    "    estimator_df = pd.DataFrame([{\n",
    "                            \"Best Estimator\" : param_search.best_estimator_,\n",
    "                            \"Best Params\" : param_search.best_params_, \n",
    "                            \"Best Score\" : param_search.best_score_,\n",
    "                            \"Search Type\" : search_type_str,\n",
    "                            \"Seed\" : random_state,\n",
    "                            \"Train F1 Weighted \" : train_report_df.loc[\"weighted avg\", \"f1-score\"],\n",
    "                            \"Test F1 Weighted \" : val_report_df.loc[\"weighted avg\", \"f1-score\"],\n",
    "                            \"Y Val\" : np.array(y_val),\n",
    "                            \"Y Pred\" : y_pred_val\n",
    "                    }])\n",
    "    \n",
    "    \n",
    "    return estimator_df, train_report_df, val_report_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paralell Tune Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def outer_tune_loop(model, param_dist, n_iter=None,target_col=\"previous_concussions\", elastic_net=False, scoring=\"F1_unweighted\", parallel=True, search_type=\"random\"):\n",
    "    start = time()\n",
    "    model_name = type(model).__name__\n",
    "    if elastic_net == True:\n",
    "        if \"Bag\" in model_name:\n",
    "            model_name = \"BaggingClassifier_ElasticNet\"\n",
    "        elif \"Ada\" in model_name:\n",
    "            model_name = \"AdaBoostClassifier_ElasticNet\"\n",
    "        else:\n",
    "            model_name = \"ElasticNet\"\n",
    "            \n",
    "    elif \"Bagging\" in model_name or \"Ada\" in model_name:\n",
    "        model_name = model_name + model.estimator.__class__.__name__\n",
    "        \n",
    "    model_name = model_name + \"_\" + search_type + \"search\"\n",
    "        \n",
    "\n",
    "    # random_states = [1, 10, 42, 69, 77, 11, 23, 99, 58, 91]\n",
    "    random_states = [10, 69, 23, 42, 89]\n",
    "#     random_states = [42, 89]\n",
    "#     random_states = [42]\n",
    "\n",
    "    estimator_dfs = pd.DataFrame()\n",
    "    training_report_dfs = pd.DataFrame()\n",
    "    val_report_dfs = pd.DataFrame()\n",
    "\n",
    "    if parallel:\n",
    "        print(\"Parallelizing with Ray\")\n",
    "        result_ids = [tune_model.remote(clone(model), param_dist, n_iter=n_iter, target_col=target_col, random_state=random_states[i], scoring=scoring, search_type=search_type) for i in range(len(random_states))]\n",
    "        print(f\"len result ids: {len(result_ids)}\")\n",
    "\n",
    "        for estimator_df, training_report_df, val_report_df in ray.get(result_ids):\n",
    "\n",
    "    #         collect each of the three dfs for one seed\n",
    "            estimator_dfs = pd.concat([estimator_dfs, estimator_df])\n",
    "            training_report_dfs = pd.concat([training_report_dfs, training_report_df])\n",
    "            val_report_dfs = pd.concat([val_report_dfs, val_report_df])  \n",
    "          \n",
    "    else:\n",
    "        print(\"Running Sequentially\")\n",
    "        for seed in random_states:\n",
    "            estimator_df, training_report_df, val_report_df = tune_model_sequential(clone(model), param_dist, n_iter=n_iter, target_col=target_col, random_state=seed, scoring=scoring, search_type=search_type)\n",
    "\n",
    "        #         collect each of the three dfs for one seed\n",
    "            estimator_dfs = pd.concat([estimator_dfs, estimator_df])\n",
    "            training_report_dfs = pd.concat([training_report_dfs, training_report_df])\n",
    "            val_report_dfs = pd.concat([val_report_dfs, val_report_df])    \n",
    "\n",
    "\n",
    "    \n",
    "#     val_f1 = round(outer_loop_results['Validation F1'].mean(), 3)\n",
    "#     val_precision = round(outer_loop_results['Validation Precision'].mean(), 3)\n",
    "#     val_recall = round(outer_loop_results['Validation Recall'].mean(), 3)\n",
    "\n",
    "#     print(f\"Mean Validation Score Across {len(random_states)} trials: {val_f1}\")\n",
    "#     print(f\"Mean Validation Precision Across {len(random_states)} trials: {val_precision}\")\n",
    "#     print(f\"Mean Validation Recall Across {len(random_states)} trials: {val_recall}\")\n",
    "   \n",
    "        \n",
    "#     outer_loop_results.to_excel(f\"../{RESULTS_DIR}{model_name}.xlsx\", index=False)\n",
    "#     print(f\"Training time for {len(random_states)} seeds: {round(time() - start, 3)}\")\n",
    "#     return outer_loop_results\n",
    "\n",
    "    estimator_dfs.to_excel(f\"../{RESULTS_DIR}{model_name}-estimator_df.xlsx\", index=True)\n",
    "    training_report_dfs.to_excel(f\"../{RESULTS_DIR}{model_name}-train_df.xlsx\", index=True)\n",
    "    val_report_dfs.to_excel(f\"../{RESULTS_DIR}{model_name}-val_df.xlsx\", index=True)\n",
    "    \n",
    "    return estimator_dfs, training_report_dfs.groupby(training_report_dfs.index).mean(), val_report_dfs.groupby(val_report_dfs.index).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPE Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "param_dist = {\n",
    "        'scaler' : CustomDistribution([myStandardScaler]),\n",
    "        \"selector\" : CustomDistribution([RandomUnderSampler(random_state=42)]),\n",
    "        \"model__criterion\" : optuna.distributions.CategoricalDistribution([\"gini\"]),\n",
    "        \"model__max_depth\" : optuna.distributions.IntDistribution(2, 8),\n",
    "        \"model__min_samples_split\" : optuna.distributions.IntDistribution(8, 15),\n",
    "        \"model__min_samples_leaf\" : optuna.distributions.IntDistribution(8, 15),\n",
    "        \"model__max_features\" : optuna.distributions.CategoricalDistribution([\"sqrt\"]),\n",
    "        \"model__class_weight\" : optuna.distributions.CategoricalDistribution([\"balanced\"]),\n",
    "    }\n",
    "\n",
    "dt_est_tpe, dt_train_tpe, dt_val_tpe = outer_tune_loop(model=model, param_dist=param_dist,scoring=\"F1_weighted\" ,n_iter=100, target_col=\"previous_concussions\", parallel=False, search_type=\"tpe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPE Decision Tree Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "display(dt_train_tpe, dt_val_tpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "param_dist = {\n",
    "        'scaler' : [myStandardScaler],\n",
    "        \"selector\" : [\n",
    "#             None\n",
    "            RandomUnderSampler(random_state=42),\n",
    "#             SelectFromModel(LinearSVC(random_state=SEED, penalty=\"l1\", dual=False)),\n",
    "#             SelectFromModel(ExtraTreesClassifier(random_state=SEED)), \n",
    "            NamedFeatureSelector([\"Delta_MT\", \"Delta_AE\", \"cvRT_HR\", \"Corrective_HR\", \"RT_V\", \"age\", \"NHL\"]),\n",
    "        ],\n",
    "        \"model__criterion\" : [\"gini\"],\n",
    "        \"model__max_depth\" : [4, 5],\n",
    "        \"model__min_samples_split\" : [13],\n",
    "        \"model__min_samples_leaf\" : [11],\n",
    "        \"model__max_features\" : [\"sqrt\"],\n",
    "        \"model__class_weight\" : [\"balanced\"],\n",
    "    }\n",
    "\n",
    "dt_est, dt_train, dt_val = outer_tune_loop(model=model, param_dist=param_dist,scoring=\"F1_weighted\" ,n_iter=100,target_col=\"previous_concussions\", parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "display(dt_train, dt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=SEED), random_state=SEED)\n",
    "param_dist = {\n",
    "        \"selector\" : [\n",
    "            RandomUnderSampler(random_state=42),\n",
    "#             None,\n",
    "#             SelectFromModel(LinearSVC(random_state=SEED, penalty=\"l1\", dual=False)),\n",
    "#             SelectFromModel(ExtraTreesClassifier(random_state=SEED)), \n",
    "            NamedFeatureSelector([\"Delta_MT\", \"Delta_AE\", \"cvRT_HR\", \"Corrective_HR\", \"RT_V\", \"age\", \"NHL\"]),\n",
    "        ],\n",
    "        'scaler' : [myMinMaxScaler, myStandardScaler],\n",
    "        \"model__n_estimators\" : [5, 10, 50, 100, 200],\n",
    "        \"model__estimator__criterion\" : [\"entropy\"],\n",
    "        \"model__estimator__max_depth\" : range(1, 5),\n",
    "        \"model__estimator__min_samples_split\" : [6],\n",
    "        \"model__estimator__min_samples_leaf\" : [4],\n",
    "        \"model__estimator__max_features\" : [\"sqrt\"],\n",
    "        \"model__estimator__class_weight\" : [\"balanced\"]\n",
    "    }\n",
    "\n",
    "\n",
    "bagged_dt_est, bagged_dt_train, bagged_dt_val = outer_tune_loop(model=model, param_dist=param_dist,scoring=\"F1_weighted\" ,n_iter=250,target_col=\"previous_concussions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagged Decision Tree Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "display(bagged_dt_est, bagged_dt_train, bagged_dt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=SEED), random_state=SEED)\n",
    "param_dist = {\n",
    "        \"selector\" : [\n",
    "            RandomUnderSampler(random_state=42)\n",
    "#             None,\n",
    "#             SelectFromModel(LinearSVC(random_state=SEED, penalty=\"l1\", dual=False)),\n",
    "#             SelectFromModel(ExtraTreesClassifier(random_state=SEED)), \n",
    "#             NamedFeatureSelector([\"Delta_MT\", \"Delta_AE\", \"cvRT_HR\", \"Corrective_HR\", \"RT_V\", \"age\"]),\n",
    "        ],\n",
    "        'scaler' : [myStandardScaler, myMinMaxScaler, myRobustScaler, None],\n",
    "        \"model__n_estimators\" : [5, 10, 50],\n",
    "        \"model__estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "        \"model__estimator__max_depth\" : range(1, 7),\n",
    "        \"model__estimator__min_samples_split\" : range(2, 7),\n",
    "        \"model__estimator__min_samples_leaf\" : range(1, 7),\n",
    "        \"model__estimator__max_features\" : [\"sqrt\", \"log2\", None],\n",
    "        \"model__estimator__class_weight\" : [\"balanced\"]\n",
    "    }\n",
    "\n",
    "\n",
    "boosted_dt_est, boosted_dt_train, boosted_dt_val  = outer_tune_loop(model=model, param_dist=param_dist, n_iter=100,target_col=\"previous_concussions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosted Decision Tree Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "display(boosted_dt_est, boosted_dt_train, boosted_dt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Collect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# ----- Random Search -----\n",
    "# weak classifier results\n",
    "estimator_results = pd.DataFrame()\n",
    "train_results = pd.DataFrame()\n",
    "val_results = pd.DataFrame()\n",
    "\n",
    "# ensemble results\n",
    "ensemble_estimator_results = pd.DataFrame()\n",
    "ensemble_train_results = pd.DataFrame()\n",
    "ensemble_val_results = pd.DataFrame()\n",
    "\n",
    "# ----- Tree-Structured Parzen Estimator Search -----\n",
    "tpe_estimator_results = pd.DataFrame()\n",
    "tpesearch_train_results = pd.DataFrame()\n",
    "tpesearch_val_results = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(\"../results\"):\n",
    "    if \"xlsx\" in file_name:\n",
    "        df = pd.read_excel(f\"../results/{file_name}\", index_col=0)\n",
    "        df[\"model\"] = file_name.split(\"_\")[0]\n",
    "        df['model'] = df['model'].str.replace('-train', '').str.replace('-val', '')\n",
    "        df = df.set_index(\"model\", append=True).swaplevel(0, 1, axis=0)\n",
    "        df.index = df.index.set_names([\"model\", \"metric type\"])\n",
    "        df = df.rename(index={'0': 'No Prior Concussion: 0'})\n",
    "        df = df.rename(index={'1': 'Prior Concussion: 1'})\n",
    "        df = df.rename(index={\"0.0\": 'No Prior Concussion: 0'})\n",
    "        df = df.rename(index={\"1.0\": 'Prior Concussion: 1'})\n",
    "#         df.rename_axis(index=['model', 'metric'])\n",
    "#         df.index=[os.path.splitext(file_name)[0]] * len(df)\n",
    "\n",
    "#         df[\"Validation F1 STD\"] = df[\"Validation F1\"].std()\n",
    "        # df = df[df[\"Validation Z Score\"] < 1.7]\n",
    "        if \"estimator\" in file_name:\n",
    "            if \"tpesearch\" in file_name:\n",
    "                tpe_estimator_results = pd.concat([tpe_estimator_results, df])\n",
    "            elif \"Bagging\" in file_name or \"Ada\" in file_name:\n",
    "                ensemble_estimator_results = pd.concat([ensemble_estimator_results, df])\n",
    "            elif 'randomsearch' in file_name:\n",
    "                estimator_results = pd.concat([estimator_results, df])\n",
    "                \n",
    "        elif \"train\" in file_name:\n",
    "            \n",
    "            if \"tpesearch\" in file_name:\n",
    "              \n",
    "                tpesearch_train_results = pd.concat([tpesearch_train_results, df])   \n",
    "            elif \"Bagging\" in file_name or \"Ada\" in file_name:\n",
    "                ensemble_train_results = pd.concat([ensemble_train_results, df])\n",
    "            elif 'randomsearch' in file_name:\n",
    "                train_results = pd.concat([train_results, df])\n",
    "                \n",
    "        elif \"val\" in file_name:\n",
    "            \n",
    "            if \"tpesearch\" in file_name:\n",
    "                tpesearch_val_results = pd.concat([tpesearch_val_results, df])\n",
    "            elif \"Bagging\" in file_name or \"Ada\" in file_name:\n",
    "                ensemble_val_results = pd.concat([ensemble_val_results, df])\n",
    "            elif 'randomsearch' in file_name:\n",
    "                print(file_name)\n",
    "                val_results = pd.concat([val_results, df])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Model and Ensemble Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def sort_by_metric(df, metric_type=\"weighted avg\", metric=\"f1-score\"):\n",
    "#     temp = df\n",
    "\n",
    "#     filter df to get order of (metric_type, metric)\n",
    "    temp = df.reset_index()\n",
    "    temp = temp[temp[\"metric type\"] == metric_type].sort_values(metric, ascending=False)\n",
    "    model_indices = {model: index for index, model in enumerate(temp['model'])}\n",
    "\n",
    "    \n",
    "#     set a new temp column that has ordering \n",
    "    df = df.reset_index(\"model\")\n",
    "    for key, value in model_indices.items():\n",
    "        df.loc[df[\"model\"] == key,  \"test index\"] = value\n",
    "\n",
    "#         sort df on temp column and reset df to how it was\n",
    "    df = df.reset_index().set_index([\"model\", \"metric type\"]).sort_values([\"test index\", \"metric type\"]).drop(columns=[\"test index\"])\n",
    "    \n",
    "#     def sort_group(group):\n",
    "#         return group.sort_values(by='metric type')\n",
    "\n",
    "    \n",
    "#     df.index = df.index.sortlevel(1, ascending=False)\n",
    "#     df.sort_index(level='metric type', sort_remaining=False, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "METRIC_TYPE = \"weighted avg\"\n",
    "METRIC = \"f1-score\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG TPE Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_tpe_train_results = tpesearch_train_results.groupby(tpesearch_train_results.index.names, group_keys=False).mean()\n",
    "avg_tpe_train_results = sort_by_metric(avg_tpe_train_results, METRIC_TYPE, METRIC)\n",
    "avg_tpe_train_results.to_excel(\"../mean_results/avg_tpe_train_results_80_20.xlsx\", index=True)\n",
    "avg_tpe_train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG TPE Val Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_tpe_val_results = tpesearch_val_results.groupby(tpesearch_val_results.index.names, group_keys=False).mean()\n",
    "avg_tpe_val_results = sort_by_metric(avg_tpe_val_results, METRIC_TYPE, METRIC)\n",
    "avg_tpe_val_results.to_excel(\"../mean_results/avg_tpe_train_results_80_20.xlsx\", index=True)\n",
    "avg_tpe_val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG TPE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "avg_tpe_results = pd.concat([avg_tpe_train_results, avg_tpe_val_results], axis=1, keys=[\"train\", 'generalize'])\n",
    "# weighted_avg_rows = pd.IndexSlice[avg_tpe_results.index.get_level_values(1) == \"weighted avg\"]\n",
    "# and apply styling to it via the `subset` arg; first arg is styler function above\n",
    "# avg_tpe_results = avg_tpe_results.style.applymap(df_style, subset=weighted_avg_rows)\n",
    "avg_tpe_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_train_results = train_results.groupby(train_results.index.names, group_keys=False).mean()\n",
    "avg_train_results = sort_by_metric(avg_train_results, METRIC_TYPE, METRIC)\n",
    "avg_train_results.to_excel(\"../mean_results/avg_train_results_80_20.xlsx\", index=True)\n",
    "avg_train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Val Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_val_results = val_results.groupby(val_results.index.names, group_keys=False).mean()\n",
    "avg_val_results = sort_by_metric(avg_val_results, METRIC_TYPE, METRIC)\n",
    "avg_val_results.to_excel(\"../mean_results/avg_val_results_80_20.xlsx\", index=True)\n",
    "avg_val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_results = pd.concat([avg_train_results, avg_val_results], axis=1, keys=[\"train\", 'generalize'])\n",
    "avg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search Paper Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "paper_avg_rs_results_df = avg_results.loc[(slice(None), 'weighted avg'), :].droplevel('metric type').drop('support', axis = 1, level = 1)\n",
    "paper_avg_rs_results_df.loc[\"Mean\"] = paper_avg_rs_results_df.mean()\n",
    "paper_avg_rs_results_df = paper_avg_rs_results_df.round(3)\n",
    "display(paper_avg_rs_results_df)\n",
    "print(paper_avg_rs_results_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TPE Search Paper Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "paper_avg_tpe_results_df = avg_tpe_results.loc[(slice(None), 'weighted avg'), :].droplevel('metric type').drop('support', axis = 1, level = 1)\n",
    "paper_avg_tpe_results_df.loc[\"Mean\"] = paper_avg_tpe_results_df.mean()\n",
    "paper_avg_tpe_results_df = paper_avg_tpe_results_df.round(3)\n",
    "display(paper_avg_tpe_results_df)\n",
    "print(paper_avg_tpe_results_df.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Ensemble Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_ensemble_train_results = ensemble_train_results.groupby(train_results.index.names, group_keys=False).mean()\n",
    "avg_ensemble_train_results = sort_by_metric(avg_ensemble_train_results, METRIC_TYPE, METRIC)\n",
    "avg_ensemble_train_results.to_excel(\"../mean_results/avg_ensemble_train_results_80_20.xlsx\", index=True)\n",
    "avg_ensemble_train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Ensemble Val Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_ensemble_val_results = ensemble_val_results.groupby(train_results.index.names, group_keys=False).mean()\n",
    "avg_ensemble_val_results = sort_by_metric(avg_ensemble_val_results, METRIC_TYPE, METRIC)\n",
    "avg_ensemble_val_results.to_excel(\"../mean_results/avg_ensemble_val_results_80_20.xlsx\", index=True)\n",
    "avg_ensemble_val_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AVG Ensemble Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_ensemble_results = pd.concat([avg_ensemble_train_results, avg_ensemble_val_results], axis=1, keys=[\"train\", 'val'])\n",
    "avg_ensemble_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Results Paper Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "avg_ensemble_results\n",
    "\n",
    "paper_avg_ensenble_results = avg_ensemble_results.loc[(slice(None), 'weighted avg'), :].droplevel('metric type').drop('support', axis = 1, level = 1)\n",
    "paper_avg_ensenble_results.loc[\"Mean\"] = paper_avg_ensenble_results.mean()\n",
    "paper_avg_ensenble_results = paper_avg_ensenble_results.round(3)\n",
    "display(paper_avg_ensenble_results)\n",
    "print(paper_avg_ensenble_results.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search Explainable Vs BlackBox Random Search Paper Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# explainable_models = [\"DecisionTreeClassifier\", \"LinearTreeClassifier\", \"RandomForestClassifier\", \"ElasticNet\", \"SVC\", \"LogisticRegression\"]\n",
    "explainable_models = [\"DecisionTreeClassifier\", \"LinearTreeClassifier\", \"RandomForestClassifier\", \"ElasticNet\", \"LogisticRegression\"]\n",
    "black_box_models = [\"LinearBoostClassifier\", \"NeuralNetClassifier\", \"LGBMClassifier\", \"XGBClassifier\"]\n",
    "\n",
    "# .drop(columns=[\"support\", \"precision\", \"recall\", \"metric type\"])\n",
    "all_val_results = avg_val_results.reset_index(\"metric type\")\n",
    "all_val_results = all_val_results[all_val_results[\"metric type\"] == \"weighted avg\"].drop(columns=[\"support\", \"precision\", \"recall\", \"metric type\"])\n",
    "avg_explainable_val_results = all_val_results.loc[explainable_models].sort_values(by=\"f1-score\", ascending=False).reset_index()\n",
    "black_box_val_results = all_val_results.loc[black_box_models].sort_values(by=\"f1-score\", ascending=False).reset_index()\n",
    "\n",
    "# all_val_results.loc[[\"DecisionTreeClassifier\", \"SVC\"]]\n",
    "explainble_vs_black_box_table_df = pd.concat([avg_explainable_val_results, black_box_val_results], keys=[\"Explainable\", \"Black Box\"], axis=1)\n",
    "\n",
    "means = explainble_vs_black_box_table_df.xs('f1-score', axis=1, level=1).mean().to_frame().transpose()\n",
    "\n",
    "explainble_vs_black_box_table_df.loc[len(explainble_vs_black_box_table_df.index)] = [\"Mean\", float(means[\"Explainable\"]), \"Mean\", float(means[\"Black Box\"])]\n",
    "explainble_vs_black_box_table_df = explainble_vs_black_box_table_df.round(3)\n",
    "display(explainble_vs_black_box_table_df)\n",
    "print(explainble_vs_black_box_table_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPE Explainable vs Black Box TPE Paper Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainable_models = [\"DecisionTreeClassifier\", \"RandomForestClassifier\", \"ElasticNet\", \"SVC\", \"LogisticRegression\"]\n",
    "black_box_models = [\"LinearBoostClassifier\", \"NeuralNetClassifier\", \"LGBMClassifier\", \"XGBClassifier\"]\n",
    "\n",
    "# .drop(columns=[\"support\", \"precision\", \"recall\", \"metric type\"])\n",
    "all_val_results = avg_tpe_val_results.reset_index(\"metric type\")\n",
    "all_val_results = all_val_results[all_val_results[\"metric type\"] == \"weighted avg\"].drop(columns=[\"support\", \"precision\", \"recall\", \"metric type\"])\n",
    "avg_explainable_val_results = all_val_results.loc[explainable_models].sort_values(by=\"f1-score\", ascending=False).reset_index()\n",
    "black_box_val_results = all_val_results.loc[black_box_models].sort_values(by=\"f1-score\", ascending=False).reset_index()\n",
    "\n",
    "# all_val_results.loc[[\"DecisionTreeClassifier\", \"SVC\"]]\n",
    "explainble_vs_black_box_table_df = pd.concat([avg_explainable_val_results, black_box_val_results], keys=[\"Explainable\", \"Black Box\"], axis=1)\n",
    "\n",
    "means = explainble_vs_black_box_table_df.xs('f1-score', axis=1, level=1).mean().to_frame().transpose()\n",
    "\n",
    "explainble_vs_black_box_table_df.loc[len(explainble_vs_black_box_table_df.index)] = [\"Mean\", float(means[\"Explainable\"]), \"Mean\", float(means[\"Black Box\"])]\n",
    "explainble_vs_black_box_table_df = explainble_vs_black_box_table_df.round(3)\n",
    "display(explainble_vs_black_box_table_df)\n",
    "print(explainble_vs_black_box_table_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from tabulate import tabulate\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import f_oneway\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random_search_val_scores = val_results.loc[(slice(None), 'weighted avg'), :].droplevel('metric type').drop(columns=['support', 'precision', 'recall'], axis = 1).reset_index()\n",
    "random_search_val_scores = random_search_val_scores[~random_search_val_scores[\"model\"].isin([\"XGBClassifier\", \"NeuralNetClassifier\"])]\n",
    "display(random_search_val_scores)\n",
    "model_scores = {}\n",
    "p_statistics = []\n",
    "for model in random_search_val_scores[\"model\"].unique():\n",
    "#     print(model)\n",
    "    \n",
    "    scores = random_search_val_scores.set_index(\"model\").loc[model].values.flatten()\n",
    "    S, p_shapiro = shapiro(scores)\n",
    "    p_statistics.append(p_shapiro)\n",
    "#     print(f\"{model} is normall dist: {p_shapiro}\")\n",
    "    model_scores[model] = scores\n",
    "    \n",
    "\n",
    "F, p_anova = f_oneway(*model_scores.values()) \n",
    "# print(p_anova)\n",
    "\n",
    "print(\"All generalization scores are normally dist: \", all(pstat > .05 for pstat in p_statistics))\n",
    "val_data_tograph = pd.DataFrame(model_scores).melt(var_name=\"Model\", value_name=\"f1-score\")\n",
    "val_data_tograph[\"Model\"] = val_data_tograph[\"Model\"].str.replace(\"Classifier\", \"\")\n",
    "\n",
    "\n",
    "sns.set_palette(\"pastel\")\n",
    "box_plot = sns.boxplot(x='Model', y='f1-score', data=val_data_tograph, )\n",
    "plt.text(0.7, .9, f\"p={round(p_anova, 4)}\", fontsize=12, ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
    "plt.title(\"Generalization Set Weighted F1-Score Distributions\")\n",
    "box_plot.set_xticklabels(box_plot.get_xticklabels(), rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPE Search ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_val_scores = tpesearch_val_results.loc[(slice(None), 'weighted avg'), :].droplevel('metric type').drop(columns=['support', 'precision', 'recall'], axis = 1).reset_index()\n",
    "# tpe_val_scores = tpe_val_scores[~tpe_val_scores[\"model\"].isin([\"XGBClassifier\", \"NeuralNetClassifier\"])]\n",
    "display(tpe_val_scores)\n",
    "model_scores = {}\n",
    "p_statistics = []\n",
    "for model in tpe_val_scores[\"model\"].unique():\n",
    "#     print(model)\n",
    "    \n",
    "    scores = tpe_val_scores.set_index(\"model\").loc[model].values.flatten()\n",
    "    S, p_shapiro = shapiro(scores)\n",
    "    p_statistics.append(p_shapiro)\n",
    "    print(f\"{model} is normall dist: {p_shapiro}\")\n",
    "    model_scores[model] = scores\n",
    "    \n",
    "\n",
    "F, p_anova = f_oneway(*model_scores.values()) \n",
    "# print(p_anova)\n",
    "\n",
    "print(\"All generalization scores are normally dist: \", all(pstat > .05 for pstat in p_statistics))\n",
    "val_data_tograph = pd.DataFrame(model_scores).melt(var_name=\"Model\", value_name=\"f1-score\")\n",
    "val_data_tograph[\"Model\"] = val_data_tograph[\"Model\"].str.replace(\"Classifier\", \"\")\n",
    "\n",
    "\n",
    "sns.set_palette(\"pastel\")\n",
    "box_plot = sns.boxplot(x='Model', y='f1-score', data=val_data_tograph, )\n",
    "plt.text(0.7, .9, f\"p={round(p_anova, 4)}\", fontsize=12, ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
    "plt.title(\"Generalization Set Weighted F1-Score Distributions\")\n",
    "box_plot.set_xticklabels(box_plot.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get confusion matrices for DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "# Get the confusion matrices for each model\n",
    "\n",
    "def get_confusion_matrices(df, result_dir, model_title=\"DecisionTree\"):\n",
    "    models = df.index.unique()\n",
    "\n",
    "    print(\"models: \", models)\n",
    "    for model_name in models:\n",
    "        y_pred = []\n",
    "        y_val = []\n",
    "\n",
    "        model_df = df.loc[model_name]\n",
    "        if isinstance(model_df, pd.DataFrame):\n",
    "            for index, row in model_df.iterrows():\n",
    "                y_pred.extend(literal_eval(row[\"Y Pred\"].replace(' ', \",\")))\n",
    "                y_val.extend(literal_eval(row[\"Y Val\"].replace(' ', \",\")))\n",
    "\n",
    "        else:\n",
    "            y_pred = literal_eval(model_df[\"Y Pred\"].replace(' ', \",\"))\n",
    "            y_val = literal_eval(model_df[\"Y Val\"].replace(' ', \",\"))\n",
    "        \n",
    "        seaborn_conf_matrix(confusion_matrix(y_val, y_pred), model_name=model_title, result_dir=result_dir)\n",
    "\n",
    "\n",
    "# get_confusion_matrices(model_results)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "get_confusion_matrices(ensemble_estimator_results.loc[\"BaggingClassifierElasticNet\"], \"../figures/confusion_matricies\", model_title=\"Bagged Elastic Net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Best Scores and Best Params Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "# from ast import eval\n",
    "\n",
    "def strip_column_transformer(params):\n",
    "    substrings_to_remove = [\"<\", \"<__\", \">\", \"class\"]\n",
    "\n",
    "    pattern = re.compile(\"|\".join(map(re.escape, substrings_to_remove)))\n",
    "    params = pattern.sub(\"\", params)\n",
    "\n",
    "    pattern = r\"__main__.NamedFeatureSelector object at 0x\\w+\"\n",
    "    params = re.sub(pattern, \"'CustomFeatureSelector'\", params)\n",
    "    \n",
    "    pattern = r\"None\"\n",
    "    params = re.sub(pattern, \"'None'\", params)\n",
    "    \n",
    "    print(params)\n",
    "    params = eval(params)\n",
    "    \n",
    "    # print(params, type(params))\n",
    "    if \"selector\" in params.keys():\n",
    "        selector = str(params[\"selector\"])\n",
    "    else:\n",
    "        selector = \"\"\n",
    "\n",
    "    if \"scaler\" in params.keys():\n",
    "        scaler = str(params[\"scaler\"])\n",
    "    else:\n",
    "        scaler = \"\"\n",
    "    \n",
    "    if \"NamedFeatureSelector\" in selector:\n",
    "        params[\"selector\"] = \"Custom Feature Selector\"\n",
    "\n",
    "    if \"StandardScaler\" in scaler:\n",
    "        params[\"scaler\"] = \"StandardScaler\"\n",
    "    \n",
    "    if \"MinMaxScaler\" in scaler:\n",
    "        params[\"scaler\"] = \"MinMaxScaler\"\n",
    "\n",
    "    if \"RobustScaler\" in scaler:\n",
    "        params[\"scaler\"] = \"RobustScaler\"\n",
    "\n",
    "    # print(params)\n",
    "    return params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.metrics import f1_score\n",
    "import pyperclip\n",
    "# test = estimator_results.loc[\"DecisionTreeClassifier\", [\"Y Val\", \"Y Pred\"]]\n",
    "rs_best_estimator_results = estimator_results.loc[:, [\"Y Val\", \"Y Pred\", \"Best Params\"]]\n",
    "\n",
    "rs_best_estimator_results[\"f1-score\"] = rs_best_estimator_results.apply(lambda row: f1_score(literal_eval(row[\"Y Val\"].replace(' ', \",\")), \n",
    "                                                  literal_eval(row[\"Y Pred\"].replace(' ', \",\")),\n",
    "                                                  average=\"weighted\"), axis=1)\n",
    "\n",
    "rs_best_estimator_results = rs_best_estimator_results.reset_index(\n",
    "    ).drop(columns=[\"metric type\"]\n",
    "    ).sort_values(by=\"f1-score\", ascending=False\n",
    "    ).drop_duplicates(subset=['model'], keep='first')\n",
    "\n",
    "\n",
    "rs_best_params = rs_best_estimator_results.set_index(\"model\")[[\"Best Params\", \"f1-score\"]]\n",
    "rs_best_params[\"Best Params\"] = rs_best_params[\"Best Params\"].apply(strip_column_transformer)\n",
    "rs_best_params.loc[\"Mean\", \"f1-score\"] = rs_best_params[\"f1-score\"].mean()\n",
    "display(rs_best_params)\n",
    "\n",
    "rs_best_params_latex_table = rs_best_params.round(3).to_latex()\n",
    "print(rs_best_params_latex_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparams Best Score TPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpe_best_estimator_results = tpe_estimator_results.loc[:, [\"Y Val\", \"Y Pred\", \"Best Params\"]]\n",
    "\n",
    "tpe_best_estimator_results[\"f1-score\"] = tpe_best_estimator_results.apply(lambda row: f1_score(literal_eval(row[\"Y Val\"].replace(' ', \",\")), \n",
    "                                                  literal_eval(row[\"Y Pred\"].replace(' ', \",\")),\n",
    "                                                  average=\"weighted\"), axis=1)\n",
    "\n",
    "tpe_best_estimator_results = tpe_best_estimator_results.reset_index(\n",
    "    ).drop(columns=[\"metric type\"]\n",
    "    ).sort_values(by=\"f1-score\", ascending=False\n",
    "    ).drop_duplicates(subset=['model'], keep='first')\n",
    "\n",
    "\n",
    "tpe_best_params = tpe_best_estimator_results.set_index(\"model\")[[\"Best Params\", \"f1-score\"]]\n",
    "tpe_best_params[\"Best Params\"] = tpe_best_params[\"Best Params\"].apply(strip_column_transformer)\n",
    "tpe_best_params.loc[\"Mean\", \"f1-score\"] = tpe_best_params[\"f1-score\"].mean()\n",
    "display(tpe_best_params)\n",
    "\n",
    "tpe_best_params_latex_table = tpe_best_params.round(3).to_latex()\n",
    "print(tpe_best_params_latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from ast import literal_eval\n",
    "from sklearn.pipeline import make_pipeline\n",
    "svm_results = tpe_estimator_results.loc[\"SVC\"]\n",
    "svm_results[\"f1-score\"] = svm_results.apply(lambda row: f1_score(literal_eval(row[\"Y Val\"].replace(' ', \",\")), \n",
    "                                                  literal_eval(row[\"Y Pred\"].replace(' ', \",\")),\n",
    "                                                  average=\"weighted\"), axis=1)\n",
    "\n",
    "best_svm = svm_results.sort_values(by=\"f1-score\", ascending=False).iloc[0].to_frame().transpose()\n",
    "test = best_svm[\"Best Estimator\"].values[0]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "            ('scaler', myStandardScaler),\n",
    "            ('selector', NamedFeatureSelector([\"Delta_MT\", \"Delta_AE\", \"cvRT_HR\", \"Corrective_HR\", \"RT_V\", \"age\", \"NHL\"])),\n",
    "            ('model',\n",
    "                 SVC(C=1.551, class_weight='balanced', gamma=51,\n",
    "                     kernel='sigmoid', random_state=42))\n",
    "\n",
    "        ])\n",
    "\n",
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "# cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=random_state)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(f1_score(y_val, y_pred))\n",
    "\n",
    "\n",
    "# pca = PCA(n_components=.8, random_state=SEED)\n",
    "# pipe = Pipeline([('scaler', myStandardScaler),\n",
    "#             ('selector', NamedFeatureSelector([\"Delta_MT\", \"Delta_AE\", \"cvRT_HR\", \"Corrective_HR\", \"RT_V\", \"age\", \"NHL\"])),\n",
    "#             ('pca', pca)])\n",
    "# Xt = pipe.fit_transform(X)\n",
    "# plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
    "# plt.legend(handles=plot.legend_elements()[0], labels=['No Concussion', 'Concussion'])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_decision_boundary(clf, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    # Reshape the meshgrid points for prediction\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Make predictions on meshgrid points\n",
    "\n",
    "\n",
    "#     f1Score = f1_score(y, Z)\n",
    "#     print(f1Score)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    start_index = 30\n",
    "    end_index = start_index + 24\n",
    "    generalizeX = X[start_index : end_index, 0]\n",
    "    generalizeY = X[start_index : end_index, 1]\n",
    "    np.random.seed(SEED)\n",
    "    generalizeT = y[start_index : end_index]\n",
    "    \n",
    "    Z = clf.predict(mesh_points)\n",
    "    f1Score = f1_score(generalizeT, Z[start_index : end_index])\n",
    "    \n",
    "    print(f1Score)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.5)\n",
    "    plt.scatter(generalizeX, generalizeY, c=generalizeT, edgecolors='k')\n",
    "    plt.text(0.7, .9, f\"f1 = {round(f1Score, 3)}\", fontsize=12, ha=\"center\", va=\"center\", transform=plt.gca().transAxes)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Decision Boundary of SVM (PCA-reduced space)')\n",
    "    plt.show()\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "            ('pca', pca)])\n",
    "\n",
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "# svm_model = pipeline.named_steps['model']\n",
    "X_train = pca_pipe.fit_transform(X_train)\n",
    "\n",
    "svm_model = SVC(C=1.551, class_weight='balanced', gamma=51, kernel='sigmoid', random_state=42)\n",
    "\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "show_decision_boundary(svm_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "pca_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                     ('pca', pca)])\n",
    "\n",
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "\n",
    "X_train = pca_pipe.fit_transform(X_train)\n",
    "\n",
    "svm_model = SVC(C=1.551, class_weight='balanced', gamma=51, kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Define the equation of the separating plane\n",
    "w = svm_model.coef_[0]\n",
    "b = svm_model.intercept_[0]\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 30), np.linspace(-5, 5, 30))\n",
    "zz = (-w[0] * xx - w[1] * yy - b) / w[2]\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(x=X_train[:, 0], y=X_train[:, 1], z=X_train[:, 2], color=y_train)\n",
    "fig.add_surface(x=xx, y=yy, z=zz, showscale=False, opacity=0.8)\n",
    "\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Principal Component 1',\n",
    "    yaxis_title='Principal Component 2',\n",
    "    zaxis_title='Principal Component 3'\n",
    "))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :3]  # we only take the first three features.\n",
    "Y = iris.target\n",
    "\n",
    "#make it binary classification problem\n",
    "X = X[np.logical_or(Y==0,Y==1)]\n",
    "Y = Y[np.logical_or(Y==0,Y==1)]\n",
    "\n",
    "display(X[:5])\n",
    "display(Y[:5])\n",
    "\n",
    "model = svm.SVC(kernel='linear')\n",
    "clf = model.fit(X, Y)\n",
    "\n",
    "# The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.\n",
    "# Solve for w3 (z)\n",
    "z = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]\n",
    "\n",
    "tmp = np.linspace(-5,5,30)\n",
    "x,y = np.meshgrid(tmp,tmp)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111, projection='3d')\n",
    "ax.plot3D(X[Y==0,0], X[Y==0,1], X[Y==0,2],'ob')\n",
    "# ax.plot3D(X[Y==1,0], X[Y==1,1], X[Y==1,2],'sr')\n",
    "ax.plot_surface(x, y, z(x,y))\n",
    "ax.view_init(30, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(pipeline, X_val, y_val, n_repeats=10, random_state=42)\n",
    "feature_importances = result.importances_mean\n",
    "feature_names = [\"Delta_MT\", \"Delta_AE\", \"cvRT_HR\", \"Corrective_HR\", \"RT_V\", \"age\", \"NHL\"]\n",
    "feature_importances = [i for i in feature_importances if i != 0]\n",
    "print(feature_names)\n",
    "print(feature_importances)\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(y='Importance', x='Feature', data=importance_df, palette=\"blend:#7AB,#EDA\")\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance (Permutation Importance)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"previous_concussions\")\n",
    "pca = PCA(n_components=.8, random_state=SEED)\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "Xt = pipe.fit_transform(X)\n",
    "plot = plt.scatter(Xt[:,0], Xt[:,1], c=y)\n",
    "plt.legend(handles=plot.legend_elements()[0], labels=['No Concussion', 'Concussion'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTES / Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna Scratch Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # -- Instantiate scaler\n",
    "    # (a) List scalers to chose from\n",
    "    scalers = trial.suggest_categorical(\"scalers\", ['minmax', 'standard', 'robust'])\n",
    "\n",
    "    # (b) Define your scalers\n",
    "    if scalers == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scalers == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "    # -- Instantiate dimensionality reduction\n",
    "     # (a) List all dimensionality reduction options\n",
    "    dim_red = trial.suggest_categorical(\"dim_red\", [\"PCA\", None])\n",
    "\n",
    "    # (b) Define the PCA algorithm and its hyperparameters\n",
    "    if dim_red == \"PCA\":\n",
    "        pca_n_components=trial.suggest_int(\"pca_n_components\", 2, 30) # suggest an integer from 2 to 30\n",
    "        dimen_red_algorithm=PCA(n_components=pca_n_components)\n",
    "    # (c) No dimensionality reduction option\n",
    "    else:\n",
    "        dimen_red_algorithm='passthrough'\n",
    "\n",
    "    # -- Instantiate estimator model\n",
    "    knn_n_neighbors=trial.suggest_int(\"knn_n_neighbors\", 1, 19, 2) # suggest an integer from 1 to with step 2\n",
    "    knn_metric=trial.suggest_categorical(\"knn_metric\", ['euclidean', 'manhattan', 'minkowski'])\n",
    "    knn_weights=trial.suggest_categorical(\"knn_weights\", ['uniform', 'distance'])\n",
    "\n",
    "    estimator=KNeighborsClassifier(n_neighbors=knn_n_neighbors, metric=knn_metric, weights=knn_weights)\n",
    "\n",
    "    # -- Make a pipeline\n",
    "    pipeline = make_pipeline(scaler, dimen_red_algorithm, estimator)\n",
    "\n",
    "    # -- Evaluate the score by cross-validation\n",
    "    score = cross_val_score(pipeline, X, y, scoring='f1')\n",
    "    f1 = score.mean() # calculate the mean of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"NHL\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=42)\n",
    "\n",
    "# Create a study name:\n",
    "study_name = 'experiment-C'\n",
    "\n",
    "# Store in DB:\n",
    "study = optuna.create_study(study_name=\"test-DT\",)\n",
    "\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "\n",
    "    scalers = trial.suggest_categorical(\"scalers\", ['minmax', 'standard', 'robust'])\n",
    "\n",
    "    # (b) Define your scalers\n",
    "    if scalers == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scalers == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    else:\n",
    "        scaler = RobustScaler()\n",
    "        \n",
    "    classifier_name = trial.suggest_categorical(\"classifier\", [\"RandomForest\"])\n",
    "    \n",
    "    # Step 2. Setup values for the hyperparameters:\n",
    "    if classifier_name == 'LogReg':\n",
    "        logreg_c = trial.suggest_float(\"logreg_c\", 1e-10, 1e10, log=True)\n",
    "        classifier_obj = linear_model.LogisticRegression(C=logreg_c)\n",
    "    else:\n",
    "        rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 10, 1000)\n",
    "        rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True)\n",
    "        classifier_obj = RandomForestClassifier(\n",
    "            max_depth=rf_max_depth, n_estimators=rf_n_estimators\n",
    "        )\n",
    "\n",
    "    pipeline = make_pipeline(scaler, classifier_obj)\n",
    "    # Step 3: Scoring method:\n",
    "    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=cv, scoring=make_scorer(f1_score, average=\"weighted\"))\n",
    "    weighted_f1 = score.mean()\n",
    "    return weighted_f1\n",
    "\n",
    "    # Step 4: Running it\n",
    "#     study = joblib.load('experiments.pkl')\n",
    "# study.optimize(objective, n_trials=3)\n",
    "#     joblib.dump(study, 'experiments.pkl')\n",
    "# study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"NHL\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "#     cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=42)\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=SEED)\n",
    "    pipe = Pipeline([(\"scaler\", myStandardScaler), (\"model\", model)])\n",
    "    \n",
    "#     max_depth = \n",
    "#     solver = trial.suggest_categorical(\"solver\", (\"lbfgs\", \"saga\"))\n",
    "    \n",
    "    params = {\n",
    "        \"model__max_depth\" : trial.suggest_int(\"model__max_depth\", 1, 20, log=True),\n",
    "        \"model__class_weight\" : \"balanced\"\n",
    "    }\n",
    "       \n",
    "    pipe.set_params(**params)\n",
    "    pipe.fit(X_train, y_train)\n",
    "   \n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    train_f1 = f1_score(y_train, y_pred_train, average='weighted')\n",
    "#     print(X_val, y_val)\n",
    "    y_pred_val = pipe.predict(X_val)\n",
    "    val_f1 = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "    print(f'Train: {train_f1} Val: {val_f1}')\n",
    "    return val_f1, train_f1 - val_f1\n",
    "\n",
    "# study = optuna.create_study(directions=[\"maximize\", \"minimize\"])\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# best_trials = study.best_trials\n",
    "# print(\"Best Parameters: \", best_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"NHL\")\n",
    "    \n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "\n",
    "    cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=42)\n",
    "\n",
    "\n",
    "#     C = trial.suggest_float(\"C\", 1e-7, 10.0, log=True)\n",
    "#     solver = trial.suggest_categorical(\"solver\", (\"lbfgs\", \"saga\"))\n",
    "\n",
    "#     clf = LogisticRegression(C=C, solver=solver)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     val_accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    param_distributions = {\n",
    "        \"model__max_depth\" : optuna.distributions.IntDistribution(2, 10)\n",
    "    }\n",
    "\n",
    "    \n",
    "#    score = cross_val_score(pipeline, X_train, y_train, n_jobs=-1, cv=cv, scoring=make_scorer(f1_score, average=\"weighted\"))\n",
    "    optuna_search = optuna.integration.OptunaSearchCV(pipe, param_distributions, cv=cv, refit=True, scoring=make_scorer(f1_score, average=\"weighted\"))\n",
    "    optuna_search.fit(X_train, y_train)\n",
    "    \n",
    "    val_accuracy = \n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "X, y = create_dataset(clean_raw_data(f\"../{DATA_DIR}Brdi_db_march.xlsx\"), target_col=\"NHL\")\n",
    "    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.20, random_state=42, stratify=y)\n",
    "\n",
    "cv = get_cross_validation(X_train, y_train, n_splits=10, type=\"stratified\", random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "param_distributions = {\n",
    "    \"model__max_depth\" : optuna.distributions.IntDistribution(2, 10)\n",
    "}\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", myStandardScaler), (\"model\", model)])\n",
    "optuna_search = optuna.integration.OptunaSearchCV(pipe, param_distributions)\n",
    "\n",
    "optuna_search.fit(X_train, y_train)\n",
    "\n",
    "display(optuna_search.trials_dataframe)\n",
    "# y_pred = optuna_search.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def df_style(val):\n",
    "    return \"font-weight: bold\"\n",
    "\n",
    "summary = pd.DataFrame([[0, 0, 0, 0, 0], [1620, 203, 392, 651, 2236], [1620, 203, 392, 651, 2236]],\n",
    "                       index=[\"None\", \"Total\", \"Total\"])\n",
    "\n",
    "summary = summary.reset_index()\n",
    "display(summary)\n",
    "last_row = summary.index[summary[\"index\"] == \"Total\"]\n",
    "print(last_row)\n",
    "# # get a handle on the row that starts with `\"Total\"`, i.e., the last row here\n",
    "# last_row = pd.IndexSlice[summary.index[summary.index == \"Total\"], :]\n",
    "# # and apply styling to it via the `subset` arg; first arg is styler function above\n",
    "summaryStyled = summary.style.applymap(df_style, subset=(slice(0, len(), 2), [\"f1-score\"]))\n",
    "\n",
    "display(summaryStyled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def df_style(val):\n",
    "    return \"font-weight: bold\"\n",
    "\n",
    "\n",
    "# tpesearch_val_results = tpesearch_val_results.style.applymap(df_style, subset=(slice(0, len(tpesearch_val_results), 5), [\"f1-score\"]))\n",
    "avg_tpe_val_results = tpesearch_val_results.groupby(tpesearch_val_results.index.names, group_keys=False).mean()\n",
    "avg_tpe_val_results = sort_by_metric(avg_tpe_val_results, METRIC_TYPE, METRIC)\n",
    "# avg_tpe_val_results = avg_tpe_val_results.style.applymap(df_style, subset=(slice(0, len(test), 4), [\"f1-score\", \"recall\" ,\"precision\"])).data.set_index([\"model\", \"metric type\"])\n",
    "avg_tpe_val_results.to_excel(\"../mean_results/avg_tpe_train_results_80_20.xlsx\", index=True)\n",
    "# avg_tpe_val_results = \n",
    "avg_tpe_val_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel",
   "language": "python",
   "name": "kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
